{
  "$schema": "https://aka.ms/codetour-schema",
  "title": "4 - Query & Retrieval: Finding Relevant Context",
  "description": "Deep dive into how user queries are processed, embedded, and matched against documents to find the most relevant context for RAG.",
  "nextTour": "5 - Response Generation: LLM Chains & Prompts",
  "steps": [
    {
      "title": "Query & Retrieval Overview",
      "description": "## Welcome to Tour 4: Query & Retrieval! ðŸ”\n\n### What We'll Cover:\n1. How user questions are processed\n2. Query embedding generation\n3. Vector similarity search mechanics\n4. Context ranking and selection\n5. Retrieval evaluation and optimization\n\n### The Retrieval Pipeline:\n```\nðŸ‘¤ \"What's the pet policy?\"\n     â†“\nðŸ§® Query Embedding: [0.23, -0.45, 0.67, ...]\n     â†“\nðŸ” Vector Search: Compare to all document embeddings\n     â†“\nðŸ“Š Rank by Similarity: Score each document\n     â†“\nðŸ“‹ Top-K Selection: Pick best 3 matches\n     â†“\nâœ… Context Retrieved: \"Pets: Up to 2 allowed...\"\n```\n\n### Why This Matters:\n- **Good retrieval = accurate answers**\n- **Poor retrieval = hallucinated responses**\n- Retrieval is often the bottleneck in RAG!\n\n---\n\n### ðŸ’¡ Beginner: The Search Analogy\n\nTraditional Search:\n- You search \"dog food\"\n- System finds pages with words \"dog\" AND \"food\"\n- Misses \"puppy chow\", \"canine nutrition\"\n\nVector Search:\n- You search \"dog food\"\n- System understands meaning\n- Finds \"puppy chow\", \"pet nutrition\", etc.\n- Semantic understanding!\n\n### ðŸ”¬ Advanced Preview:\n- Query expansion techniques\n- Re-ranking strategies\n- Hybrid search (keywords + vectors)\n- Retrieval evaluation metrics (MRR, NDCG)\n\n---\n**Let's explore the retrieval process!**"
    },
    {
      "file": "packages/api/src/functions/chats-post.ts",
      "line": 42,
      "title": "Processing Incoming Queries",
      "description": "## Query Processing Entry Point ðŸšª\n\n### The Request:\n```typescript\nconst requestBody = await request.json() as AIChatCompletionRequest;\nconst { messages, context: chatContext } = requestBody;\n```\n\n### What We Receive:\n```json\n{\n  \"messages\": [\n    {\"role\": \"user\", \"content\": \"What's the pet policy?\"},\n    {\"role\": \"assistant\", \"content\": \"...\"},\n    {\"role\": \"user\", \"content\": \"Can I have 3 dogs?\"}\n  ],\n  \"context\": {\n    \"sessionId\": \"abc-123\",\n    \"userId\": \"user-456\"\n  }\n}\n```\n\n### Extracting the Query:\n```typescript\nconst question = messages.at(-1)!.content;\n```\n\n**Why `.at(-1)`?**\n- Last message is always the current user question\n- Previous messages provide conversation history\n- We retrieve based on current question only\n\n---\n\n### ðŸ’¡ Beginner: Message Format\n\nChat format follows a pattern:\n```\nUser: \"What's the pet policy?\"\nAssistant: \"You can have up to 2 pets.\"\nUser: \"Can I have 3 dogs?\"  â† This is the current query\n```\n\nWe retrieve documents for \"Can I have 3 dogs?\"\n\nBut we ALSO include previous conversation in the prompt!\n- Retrieval: Based on current question\n- Generation: Uses full conversation history\n\n### ðŸ”¬ Advanced: Query Pre-processing Options\n\n**Not implemented here, but consider:**\n\n1. **Query Expansion**:\n```typescript\n\"pet policy\" â†’ [\"pet policy\", \"animal rules\", \"pet restrictions\"]\n// Retrieve for each, merge results\n```\n\n2. **Query Decomposition**:\n```typescript\n\"Can I have 3 dogs and 2 cats?\"\n  â†’ [\"dog limit\", \"cat limit\", \"total pet limit\"]\n// Multiple retrievals, better coverage\n```\n\n3. **Spell Correction**:\n```typescript\n\"pett policiy\" â†’ \"pet policy\"\n// Fix typos before embedding\n```\n\n4. **Language Detection**:\n```typescript\n\"Â¿CuÃ¡l es la polÃ­tica de mascotas?\"\n// Translate to English OR use multilingual embeddings\n```\n\n**Our approach:** Keep it simpleâ€”use query as-is!\n\n---\n**Next**: See how the query is embedded!"
    },
    {
      "file": "packages/api/src/functions/chats-post.ts",
      "line": 62,
      "title": "Query Embedding Generation",
      "description": "## Converting Query to Vector ðŸ§®\n\nThe embeddings model is initialized earlier:\n\n### Azure Path:\n```typescript\nconst embeddings = new AzureOpenAIEmbeddings({ \n  azureADTokenProvider \n});\n```\n\n### Local Path:\n```typescript\nconst embeddings = new OllamaEmbeddings({ \n  model: ollamaEmbeddingsModel  // \"nomic-embed-text\"\n});\n```\n\n### What Happens When We Retrieve:\n\nInside `retriever.invoke(question)`, this occurs:\n\n```typescript\n// 1. Embed the query\nconst queryVector = await embeddings.embedQuery(\n  \"Can I have 3 dogs?\"\n);\n\n// Result: Array of 768-1536 numbers\n// [0.234, -0.456, 0.789, ...]\n```\n\n### Why Same Embedding Model?\n\n**Critical:** Query and documents MUST use same embeddings model!\n\nâŒ **Wrong:**\n```\nDocuments: Azure OpenAI embeddings (1536 dims)\nQuery: Ollama embeddings (768 dims)\nâ†’ Incompatible! Can't compare!\n```\n\nâœ… **Correct:**\n```\nDocuments: Azure OpenAI embeddings (1536 dims)\nQuery: Azure OpenAI embeddings (1536 dims)\nâ†’ Compatible! Same vector space!\n```\n\n---\n\n### ðŸ’¡ Beginner: The Vector Space\n\nThink of embeddings as coordinates in a huge map:\n- Each word/phrase gets X, Y, Z coordinates (but 768-1536 dimensions!)\n- Similar meanings = nearby coordinates\n- Query embedding = \"Where am I looking?\"\n- Document embeddings = \"Where are all the documents?\"\n- Retrieval = \"Which documents are closest to where I'm looking?\"\n\n### ðŸ”¬ Advanced: Embedding Quality Factors\n\n**What makes a good embedding for retrieval?**\n\n1. **Semantic Clustering**:\n   - Similar meanings â†’ similar vectors\n   - Measured by: Clustering metrics\n\n2. **Discriminative Power**:\n   - Different meanings â†’ different vectors\n   - Measured by: Classification accuracy\n\n3. **Dimension Efficiency**:\n   - Few dimensions, high quality\n   - Measured by: Dimensionality reduction loss\n\n4. **Domain Adaptation**:\n   - Works well on your specific documents\n   - May require fine-tuning!\n\n**Fine-tuning embeddings:**\n```python\n# Not in this sample, but possible:\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('base-model')\n\n# Train on your domain data\nmodel.fit(train_data, ...)\n\n# Use for better retrieval on your docs\n```\n\n**When to fine-tune:**\n- Highly specialized domain (medical, legal)\n- Non-English languages\n- Very specific terminology\n- After measuring poor retrieval quality\n\n**For most use cases:** Pre-trained models work great!\n\n---\n**Next**: The vector search process!"
    },
    {
      "file": "packages/api/src/functions/chats-post.ts",
      "line": 110,
      "title": "Retriever Configuration",
      "description": "## Configuring the Retriever âš™ï¸\n\nThis line sets up how retrieval works:\n\n```typescript\nconst retriever = store.asRetriever(3);\n```\n\n### What This Creates:\n\nA retriever with these defaults:\n- **Search Type**: Similarity search\n- **K**: 3 (top 3 results)\n- **Filter**: None (search all documents)\n\n### Full Configuration Options:\n\n```typescript\n// Expanded version (all options):\nconst retriever = store.asRetriever({\n  searchType: 'similarity',  // or 'mmr', 'similarity_score_threshold'\n  k: 3,                      // number of results\n  filter: undefined,         // metadata filters\n  scoreThreshold: undefined, // min similarity score\n  fetchK: undefined,         // for MMR: initial fetch size\n  lambda: undefined,         // for MMR: diversity parameter\n});\n```\n\n### Search Type Options:\n\n#### 1. Similarity (Default)\n```typescript\nsearchType: 'similarity'\n```\n- Returns top-K most similar documents\n- Simple, reliable, fast\n- What we use!\n\n#### 2. MMR (Maximal Marginal Relevance)\n```typescript\nsearchType: 'mmr',\nfetchK: 20,  // Fetch 20 candidates\nlambda: 0.5, // Balance relevance (1.0) vs diversity (0.0)\nk: 3         // Return 3 final results\n```\n- Reduces redundancy\n- Returns diverse results\n- Slower but better coverage\n\n#### 3. Similarity Score Threshold\n```typescript\nsearchType: 'similarity_score_threshold',\nscoreThreshold: 0.8,  // Min score (0-1)\nk: 10                 // Max results\n```\n- Only returns if score > threshold\n- Variable number of results\n- Good for quality control\n\n---\n\n### ðŸ’¡ Beginner: The K Parameter\n\n**K = How many results to return**\n\nToo few (K=1):\n- Fast\n- Might miss important context\n- Cheap (fewer tokens)\n\nJust right (K=3-5):\n- Balanced\n- Usually enough context\n- Our choice!\n\nToo many (K=20):\n- Slow\n- Includes noise\n- Expensive (many tokens)\n\n### ðŸ”¬ Advanced: Adaptive K\n\n**Problem**: Some queries need more/less context\n\n**Solution**: Dynamically adjust K!\n\n```typescript\nfunction getAdaptiveK(query: string): number {\n  // Simple heuristic\n  if (query.includes('?')) return 5;  // Questions need more context\n  if (query.length < 20) return 2;    // Short queries, fewer results\n  return 3;                           // Default\n}\n\nconst k = getAdaptiveK(question);\nconst retriever = store.asRetriever(k);\n```\n\n**Better approach**: Learn from user feedback!\n```typescript\n// Track which K values led to good answers\nfunction getOptimalK(query: string, feedback: UserFeedback): number {\n  // ML model trained on historical data\n  return mlModel.predict(query, feedback);\n}\n```\n\n---\n**Next**: The actual search execution!"
    },
    {
      "file": "packages/api/src/functions/chats-post.ts",
      "line": 112,
      "title": "Executing Retrieval",
      "description": "## The Retrieval Execution ðŸŽ¯\n\nHere's where the magic happens:\n\n```typescript\nconst responseStream = await ragChainWithHistory.stream(\n  {\n    input: question,\n    context: await retriever.invoke(question),  // â† RETRIEVAL!\n  },\n  { configurable: { sessionId } }\n);\n```\n\n### Breaking Down `retriever.invoke(question)`:\n\n#### Step 1: Embed Query\n```typescript\nconst queryVector = await embeddings.embedQuery(\n  \"Can I have 3 dogs?\"\n);\n// Result: [0.234, -0.456, 0.789, ..., 0.123]  (1536 numbers)\n```\n\n#### Step 2: Search Vector Store\n```typescript\nconst results = await store.similaritySearchVectorWithScore(\n  queryVector,\n  3  // top-K\n);\n```\n\n**What happens in the vector store:**\n\n**For Cosmos DB:**\n```sql\nSELECT TOP 3 \n  c.text,\n  c.metadata,\n  VectorDistance(c.embedding, @queryVector) as score\nFROM c\nORDER BY VectorDistance(c.embedding, @queryVector)\n```\n\n**For FAISS:**\n```typescript\n// Compute distance to ALL vectors\nfor (const docVector of allVectors) {\n  const distance = cosineSimilarity(queryVector, docVector);\n  scores.push({ doc, distance });\n}\n// Sort and take top-3\nreturn scores.sort().slice(0, 3);\n```\n\n#### Step 3: Format Results\n```typescript\nconst documents = results.map(([doc, score]) => ({\n  pageContent: doc.text,\n  metadata: doc.metadata,\n  score: score  // 0-1, higher = more similar\n}));\n```\n\n### Example Results:\n\n```javascript\n[\n  {\n    pageContent: \"Pets: Tenants may keep up to 2 pets...\",\n    metadata: { source: \"terms.pdf\", page: 5 },\n    score: 0.89  // Very similar!\n  },\n  {\n    pageContent: \"Pet deposits: $200 per pet, refundable...\",\n    metadata: { source: \"terms.pdf\", page: 6 },\n    score: 0.76  // Somewhat similar\n  },\n  {\n    pageContent: \"Noise complaints: Excessive barking...\",\n    metadata: { source: \"guide.pdf\", page: 12 },\n    score: 0.71  // Less similar\n  }\n]\n```\n\n---\n\n### ðŸ’¡ Beginner: How Similarity Scores Work\n\n**Score range**: 0.0 to 1.0\n- 1.0 = Perfect match (exact same text)\n- 0.8-0.9 = Very similar (almost certainly relevant)\n- 0.6-0.7 = Somewhat similar (might be relevant)\n- < 0.5 = Not very similar (probably not relevant)\n\nThink of it like a percentage match!\n\n### ðŸ”¬ Advanced: Retrieval Performance\n\n**Benchmark (typical RAG system):**\n\n**Embedding:**\n- Azure OpenAI: 20-50ms\n- Ollama (CPU): 100-500ms\n- Ollama (GPU): 10-50ms\n\n**Vector Search:**\n- Cosmos DB: 10-50ms (indexed)\n- FAISS (10K vectors): 5-20ms\n- FAISS (100K vectors): 50-200ms\n\n**Total Retrieval Time:**\n- Azure: 30-100ms âœ… Fast!\n- Local (CPU): 150-700ms (acceptable)\n- Local (GPU): 20-100ms (very fast)\n\n**Optimization Strategies:**\n\n1. **Batch Queries**: If multiple users, batch embeddings\n2. **Cache Embeddings**: Cache common query embeddings\n3. **Approximate Search**: Trade accuracy for speed (HNSW index)\n4. **GPU Acceleration**: 10-50x faster embeddings\n\n**When to Optimize:**\n- Latency > 500ms\n- Cost concerns (API calls)\n- High query volume (> 1000/min)\n\n---\n**Next**: Understanding context quality!"
    },
    {
      "title": "Context Quality & Ranking",
      "description": "## Evaluating Retrieved Context ðŸ“Š\n\n### The Context Quality Problem:\n\n**Question**: \"What's the pet policy?\"\n\n**Good Retrieval**:\n1. âœ… \"Pets: Up to 2 animals allowed per unit...\"\n2. âœ… \"Pet deposits: $200 per pet, refundable...\"\n3. âœ… \"Breed restrictions: No aggressive breeds...\"\nâ†’ Answer: \"You can have up to 2 pets with a $200 deposit...\"\n\n**Poor Retrieval**:\n1. âŒ \"Parking: Each unit gets 2 spaces...\"\n2. âŒ \"Lease terms: Minimum 6 months...\"\n3. âŒ \"Maintenance requests: Submit online...\"\nâ†’ Answer: \"I don't have information about pet policies.\" (hallucination risk!)\n\n### Why Poor Retrieval Happens:\n\n1. **Query ambiguity**: \"policy\" matches many things\n2. **Embedding quality**: Model didn't capture semantics well\n3. **Chunking issues**: Relevant info split across chunks\n4. **Index coverage**: Pet policy not in database\n\n---\n\n## Retrieval Evaluation Metrics:\n\n### 1. Precision@K\n```\nPrecision@3 = (# relevant in top-3) / 3\n```\n\nExample:\n- Top 3 results: [relevant, not relevant, relevant]\n- Precision@3 = 2/3 = 0.67\n\n**What it tells you**: Are retrieved docs actually relevant?\n\n---\n\n### 2. Recall@K\n```\nRecall@3 = (# relevant in top-3) / (total # relevant in DB)\n```\n\nExample:\n- Top 3 results: 2 relevant\n- Total relevant in DB: 5\n- Recall@3 = 2/5 = 0.40\n\n**What it tells you**: Did we find all relevant docs?\n\n---\n\n### 3. Mean Reciprocal Rank (MRR)\n```\nMRR = 1 / (position of first relevant result)\n```\n\nExample:\n- First relevant at position 2\n- MRR = 1/2 = 0.5\n\n**What it tells you**: How quickly do we find relevant results?\n\n---\n\n### 4. NDCG (Normalized Discounted Cumulative Gain)\n```\nNDCG = DCG / IDCG\nDCG = Î£(relevance_i / log2(i+1))\n```\n\n**What it tells you**: Considers both relevance and ranking order\n\n---\n\n### ðŸ’¡ Beginner: Why Multiple Metrics?\n\nEach tells a different story:\n\n**High Precision**: \"When you retrieve, you get good results\"\n- Important for user trust\n\n**High Recall**: \"You find all relevant information\"\n- Important for completeness\n\n**High MRR**: \"Best results come first\"\n- Important for efficiency\n\n**You want all three high!** But trade-offs exist:\n- More results (higher K) â†’ Better recall, worse precision\n- Stricter threshold â†’ Better precision, worse recall\n\n### ðŸ”¬ Advanced: Measuring Retrieval Quality\n\n**Create a test set:**\n\n```typescript\nconst testQueries = [\n  {\n    query: \"What's the pet policy?\",\n    relevantDocs: [\"chunk-123\", \"chunk-456\"],  // Ground truth\n  },\n  {\n    query: \"How do I pay rent?\",\n    relevantDocs: [\"chunk-789\"],\n  },\n  // ... 50-100 test queries\n];\n\nasync function evaluateRetrieval() {\n  let precisionSum = 0;\n  \n  for (const test of testQueries) {\n    const retrieved = await retriever.invoke(test.query);\n    const retrievedIds = retrieved.map(doc => doc.id);\n    \n    const relevant = retrievedIds.filter(\n      id => test.relevantDocs.includes(id)\n    );\n    \n    precisionSum += relevant.length / retrievedIds.length;\n  }\n  \n  const avgPrecision = precisionSum / testQueries.length;\n  console.log(`Average Precision: ${avgPrecision}`);\n}\n```\n\n**Improving retrieval:**\n\n1. **Tune K**: Test K=1,2,3,5,10\n2. **Try MMR**: Reduce redundancy\n3. **Adjust chunking**: Test 500, 1500, 3000 char chunks\n4. **Fine-tune embeddings**: On your domain\n5. **Hybrid search**: Combine with keyword search\n\n---\n**Next**: Advanced retrieval patterns!"
    },
    {
      "title": "Advanced Retrieval Patterns",
      "description": "## Beyond Basic Retrieval ðŸš€\n\n### 1. Hybrid Search (Keyword + Vector)\n\n**Problem**: Vector search misses exact term matches\n\n**Example:**\n- Query: \"What's the policy for pets?\"\n- Vector search: Finds similar semantics âœ…\n- BUT misses document with \"pet regulation\" (exact term)\n\n**Solution**: Combine both!\n\n```typescript\n// Not implemented in this sample, but conceptual:\nasync function hybridSearch(query: string, k: number) {\n  // 1. Vector search\n  const vectorResults = await store.similaritySearch(query, k);\n  \n  // 2. Keyword search (BM25)\n  const keywordResults = await keywordSearch(query, k);\n  \n  // 3. Merge with weighted scoring\n  const combined = mergeResults(\n    vectorResults,\n    keywordResults,\n    { vectorWeight: 0.7, keywordWeight: 0.3 }\n  );\n  \n  return combined.slice(0, k);\n}\n```\n\n**When to use**: Legal, medical, technical docs with specific terminology\n\n---\n\n### 2. Re-ranking\n\n**Problem**: Initial retrieval is approximate, not perfect\n\n**Solution**: Two-stage retrieval!\n\n```typescript\n// Stage 1: Fast, approximate retrieval (get 20)\nconst candidates = await retriever.invoke(query, { k: 20 });\n\n// Stage 2: Accurate re-ranking (pick best 3)\nconst reranked = await reranker.rerank(query, candidates);\nconst topK = reranked.slice(0, 3);\n```\n\n**Re-ranking models:**\n- Cross-encoders (BERT-based)\n- Much slower but more accurate\n- 100x slower, 5-10% better accuracy\n\n**Trade-off**: 2x retrieval time, better results\n\n---\n\n### 3. Multi-Query Retrieval\n\n**Problem**: One query might miss relevant docs\n\n**Solution**: Generate multiple query variations!\n\n```typescript\nasync function multiQueryRetrieval(originalQuery: string) {\n  // Generate variations using LLM\n  const queries = await llm.invoke(\n    `Generate 3 variations of this query: \"${originalQuery}\"`\n  );\n  // Result: [\n  //   \"What's the pet policy?\",\n  //   \"Are pets allowed in apartments?\",  \n  //   \"What are the rules for animals?\"\n  // ]\n  \n  // Retrieve for each\n  const allResults = [];\n  for (const q of queries) {\n    const results = await retriever.invoke(q);\n    allResults.push(...results);\n  }\n  \n  // Deduplicate and rank\n  return deduplicateAndRank(allResults);\n}\n```\n\n**Benefit**: Better recall (find more relevant docs)\n**Cost**: 3x retrieval calls\n\n---\n\n### 4. Contextual Compression\n\n**Problem**: Retrieved chunks include irrelevant parts\n\n**Solution**: Extract only relevant sentences!\n\n```typescript\n// Retrieve full chunks\nconst chunks = await retriever.invoke(query);\n\n// Compress each chunk (remove irrelevant sentences)\nconst compressed = await compressor.compress(query, chunks);\n\n// Result: Smaller, more focused context\n// \"Pets: Up to 2 allowed.\" (was 500 words, now 10)\n```\n\n**Benefit**: \n- Fewer tokens â†’ Lower cost\n- Less noise â†’ Better answers\n\n---\n\n### 5. Parent Document Retrieval\n\n**Problem**: Retrieved chunk lacks context\n\n**Solution**: Retrieve small chunks, but return parent documents!\n\n```typescript\n// Index: Small chunks (100 chars) for precise search\n// Store: Parent chunks (1500 chars) for full context\n\nconst smallChunks = await retriever.invoke(query);  // Precise match\nconst parentDocs = smallChunks.map(chunk => \n  getParentDocument(chunk.id)  // Full context\n);\n```\n\n**Benefit**: Precision of small chunks + context of large chunks\n\n---\n\n### ðŸ’¡ Beginner: When to Use Advanced Patterns\n\n**Start simple**: Basic retrieval (what we have)\n\n**Add complexity only if:**\n- Measuring poor retrieval quality\n- User complaints about answers\n- High-stakes domain (legal, medical)\n\n**Don't over-engineer!**\n---\n**Next**: Tour complete!"
    },
    {
      "title": "Query & Retrieval Tour Complete! ðŸŽ‰",
      "description": "## Tour 4 Complete - Retrieval Mastered! âœ…\n\n### What You've Learned:\n\n#### Core Concepts:\n\n**Beginner Level:**\nâœ… Queries are embedded using the same model as documents\nâœ… Vector search finds semantically similar chunks\nâœ… Top-K selection returns most relevant results\nâœ… Retrieved context is passed to the LLM\n\n**Advanced Level:**\nâœ… Retrieval evaluation metrics (Precision, Recall, MRR)\nâœ… Hybrid search (keyword + vector)\nâœ… Re-ranking for improved accuracy\nâœ… Multi-query retrieval for better recall\nâœ… Contextual compression to reduce noise\n\n---\n\n## ðŸŽ¯ The Retrieval Pipeline:\n\n```\n1. User Query\n   \"What's the pet policy?\"\n   \n2. Query Embedding\n   [0.234, -0.456, 0.789, ...]  (1536 dims)\n   \n3. Vector Search\n   Compare to all doc embeddings\n   Score by cosine similarity\n   \n4. Ranking\n   Sort by score (0.89, 0.76, 0.71)\n   \n5. Top-K Selection\n   Return top 3 documents\n   \n6. Context Formatting\n   [source]: content\n   Ready for LLM!\n```\n\n---\n\n## ðŸ’¡ Key Takeaways:\n\n### Retrieval Quality = Answer Quality\n\n**Good retrieval:**\n- Relevant chunks â†’ Accurate answers\n- Citations â†’ User trust\n- Fast search â†’ Good UX\n\n**Poor retrieval:**\n- Irrelevant chunks â†’ Wrong answers\n- No citations â†’ Hallucinations\n- Slow search â†’ Bad UX\n\n### Optimization Priorities:\n\n```\n1. Start simple (basic similarity search)\n2. Measure quality (precision/recall)\n3. Optimize if needed (advanced patterns)\n4. Re-measure impact\n```\n\n---\n\n## ðŸš€ What's Next?\n\nNow that we have relevant context, how do we generate answers?\n\n### [Tour 5: Response Generation]\nExplore:\n- LangChain chain construction\n- Prompt engineering for RAG\n- LLM model selection and configuration\n- Handling chat history\n- Citation extraction\n\n---\n\n## ðŸ§ª Experiment Ideas:\n\n1. **Test different K values**:\n   - Try K=1,3,5,10\n   - See how answers change\n\n2. **Measure retrieval quality**:\n   - Create 10 test questions\n   - Check if retrieved docs are relevant\n   - Calculate precision\n\n3. **Compare embedding models**:\n   - Azure OpenAI vs Ollama\n   - Measure retrieval accuracy\n\n4. **Try MMR**:\n   - Change to `searchType: 'mmr'`\n   - See if results are more diverse\n\n---\n\n### Continue Learning:\n[Tour 5: Response Generation â†’]\n\n---\n\n**Further Reading:**\n- [LangChain Retrievers](https://js.langchain.com/docs/modules/data_connection/retrievers/)\n- [Vector Search Concepts](https://learn.microsoft.com/azure/cosmos-db/nosql/vector-search)\n- [RAGAS Framework](https://docs.ragas.io/)\n- [Information Retrieval Metrics](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval))"
    }
  ]
}