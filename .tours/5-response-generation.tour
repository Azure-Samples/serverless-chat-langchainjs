{
  "$schema": "https://aka.ms/codetour-schema",
  "title": "5 - Response Generation: LLM Chains & Prompts",
  "description": "Understand how LangChain chains combine retrieved context with user queries to generate accurate, grounded responses using LLMs.",
  "nextTour": "6 - Streaming & Chat History: Real-Time UX",
  "steps": [
    {
      "title": "Response Generation Overview",
      "description": "## Welcome to Tour 5: Response Generation! ğŸ¤–\n\n### What We'll Cover:\n1. How LangChain chains orchestrate RAG\n2. Prompt engineering for grounded responses\n3. LLM model selection and configuration\n4. Combining context + query + history\n5. Citation extraction and formatting\n\n### The Generation Pipeline:\n```\nğŸ“‹ Retrieved Context + ğŸ‘¤ User Query + ğŸ’¬ Chat History\n                  â†“\n           ğŸ“ Prompt Template\n                  â†“\n            ğŸ¤– LLM (GPT/Llama)\n                  â†“\n          ğŸ’¬ Generated Response\n                  â†“\n           ğŸ”— + Citations\n```\n\n### Why This Matters:\n- **Good prompt = grounded, accurate answers**\n- **Poor prompt = hallucinations, off-topic responses**\n- Generation is where RAG's value becomes visible to users!\n\n---\n\n### ğŸ’¡ Beginner: The Assembly Line Analogy\n\nThink of response generation like making a sandwich:\n1. **Context** = Ingredients (what you have available)\n2. **Query** = Order (what the customer wants)\n3. **Prompt** = Recipe (how to combine ingredients)\n4. **LLM** = Chef (does the actual cooking)\n5. **Response** = Final sandwich (what customer gets)\n\nRAG ensures the chef uses ONLY available ingredients (no making stuff up!)\n\n### ğŸ”¬ Advanced Preview:\n- Chain composition patterns\n- Few-shot prompting strategies\n- Temperature and generation parameters\n- Output parsing and validation\n\n---\n**Let's explore response generation!**"
    },
    {
      "file": "packages/api/src/functions/chats-post.ts",
      "line": 14,
      "title": "The RAG System Prompt",
      "description": "## Crafting the Perfect RAG Prompt ğŸ“\n\nThis prompt is the **most important** part of RAG!\n\n```typescript\nconst ragSystemPrompt = `Assistant helps the Consto Real Estate company customers with questions and support requests. Be brief in your answers. Answer only plain text, DO NOT use Markdown.\nAnswer ONLY with information from the sources below. If there isn't enough information in the sources, say you don't know. Do not generate answers that don't use the sources. If asking a clarifying question to the user would help, ask the question.\nIf the user question is not in English, answer in the language used in the question.\n\nEach source has the format \"[filename]: information\". ALWAYS reference the source filename for every part used in the answer. Use the format \"[filename]\" to reference a source, for example: [info1.txt]. List each source separately, for example: [info1.txt][info2.pdf].\n\nGenerate 3 very brief follow-up questions that the user would likely ask next.\nEnclose the follow-up questions in double angle brackets. Example:\n<<Am I allowed to invite friends for a party?>>\n<<How can I ask for a refund?>>\n<<What If I break something?>>\n\nDo no repeat questions that have already been asked.\nMake sure the last question ends with \">>\".\n\nSOURCES:\n{context}`;\n```\n\n### Breaking Down Each Directive:\n\n---\n\n#### 1. Context Setting\n```\n\"Assistant helps the Consto Real Estate company customers...\"\n```\n**Purpose**: Establishes role and domain\n**Effect**: LLM adopts appropriate tone and expertise\n\n---\n\n#### 2. Anti-Hallucination Instructions\n```\n\"Answer ONLY with information from the sources below.\"\n\"If there isn't enough information, say you don't know.\"\n```\n**Purpose**: Prevents making up information\n**Effect**: Grounded, trustworthy responses\n\n**Without this:**\nâŒ \"Pets are allowed, and I think you can have up to 5...\" (hallucination!)\n\n**With this:**\nâœ… \"According to the terms, up to 2 pets are allowed [terms.pdf]\"\n\n---\n\n#### 3. Citation Requirements\n```\n\"ALWAYS reference the source filename...\"\n\"Use the format [filename]...\"\n```\n**Purpose**: Enable verification\n**Effect**: Users can trust and verify answers\n\n**Example output:**\n```\nYou can have up to 2 pets [terms.pdf]. A $200 deposit per pet is \nrequired [terms.pdf]. Breed restrictions apply [policy.pdf].\n```\n\n---\n\n#### 4. Follow-up Questions\n```\n\"Generate 3 very brief follow-up questions...\"\n\"Enclose in double angle brackets...\"\n```\n**Purpose**: Improve UX, guide conversation\n**Effect**: Users know what to ask next\n\n**Example:**\n```\n<<What breeds are restricted?>>\n<<Is the pet deposit refundable?>>\n<<Can I have a service animal?>>\n```\n\n---\n\n#### 5. Multilingual Support\n```\n\"If the user question is not in English, answer in the language used...\"\n```\n**Purpose**: Accessibility\n**Effect**: Works for Spanish, French, etc.\n\n---\n\n#### 6. Context Injection Point\n```\nSOURCES:\n{context}\n```\n**This is where retrieved documents are inserted!**\n\n**Format:**\n```\n[terms.pdf]: Pets: Up to 2 animals allowed per unit...\n[policy.pdf]: Pet deposits are $200 per pet, refundable...\n[guide.pdf]: Service animals are exempt from pet limits...\n```\n\n---\n\n### ğŸ’¡ Beginner: Why So Specific?\n\nLLMs are VERY literal. Without explicit instructions:\n- They'll make up answers (hallucinate)\n- Won't cite sources\n- Might ignore context\n- Could be verbose\n\n**Clear instructions = consistent, reliable behavior!**\n\n### ğŸ”¬ Advanced: Prompt Engineering Techniques\n\n#### 1. Instruction Hierarchy\n```\n1. Critical (CAPS, repetition): \"Answer ONLY...\", \"ALWAYS reference...\"\n2. Important (clear directive): \"Be brief\", \"Generate 3 questions\"\n3. Optional (conditional): \"If there isn't enough information...\"\n```\n\n#### 2. Few-Shot Examples (Not Used Here)\n```typescript\nconst promptWithExamples = `\nAnswer questions based on sources.\n\nExample:\nQ: What's the pet policy?\nSources: [terms.pdf]: Up to 2 pets allowed\nA: You can have up to 2 pets [terms.pdf].\n\nNow answer:\nQ: {question}\nSources: {context}\nA: `;\n```\n**Benefit**: Shows desired output format\n**Trade-off**: Uses more tokens\n\n#### 3. Chain-of-Thought (Not Used Here)\n```typescript\nconst cotPrompt = `\nLet's think step by step:\n1. What is the user asking?\n2. Which sources are relevant?\n3. What information do they provide?\n4. Formulate answer with citations.\n\n{context}\nQuestion: {question}`;\n```\n**Benefit**: More accurate reasoning\n**Trade-off**: Slower, more verbose\n\n---\n**Next**: See the title generation prompt!"
    },
    {
      "file": "packages/api/src/functions/chats-post.ts",
      "line": 37,
      "title": "Title Generation Prompt",
      "description": "## Auto-Generating Chat Titles ğŸ·ï¸\n\nThis prompt creates short, descriptive titles for chat sessions:\n\n```typescript\nconst titleSystemPrompt = `Create a title for this chat session, based on the user question. The title should be less than 32 characters. Do NOT use double-quotes.`;\n```\n\n### Why Separate Titles?\n\n**UX Benefit**: Users can see their chat history:\n```\nğŸ“‹ Chat History:\n  - \"Pet policy questions\" (3 messages)\n  - \"Rent payment options\" (5 messages)\n  - \"Maintenance request\" (2 messages)\n```\n\nVs generic:\n```\nğŸ“‹ Chat History:\n  - \"Chat 1\" (3 messages)\n  - \"Chat 2\" (5 messages)  \n  - \"Chat 3\" (2 messages)\n```\n\n### How It Works:\n\n```typescript\nif (!title) {  // First message of session\n  const response = await ChatPromptTemplate\n    .fromMessages([\n      ['system', titleSystemPrompt],\n      ['human', '{input}'],\n    ])\n    .pipe(model)\n    .invoke({ input: question });\n  \n  chatHistory.setContext({ title: response.content });\n}\n```\n\n### Example:\n```\nUser: \"What's the policy on having pets in my apartment?\"\n\nLLM generates title:\n\"Pet policy inquiry\"  âœ… (short, descriptive)\n\nvs:\n\"What's the policy on having pets in my apartment?\"  âŒ (too long)\n\"Chat\"  âŒ (not descriptive)\n```\n\n---\n\n### ğŸ’¡ Beginner: Why Not Use First Message?\n\nUser questions are often:\n- Too long: \"I was wondering if you could tell me...\"\n- Too vague: \"Question about pets\"\n- Unclear: \"Is this allowed?\"\n\nLLM can summarize into clear, concise titles!\n\n### ğŸ”¬ Advanced: Constraint Optimization\n\n**Constraints in prompt:**\n1. **Length**: \"less than 32 characters\"\n   - UI display limit\n   - Database field size\n\n2. **Format**: \"Do NOT use double-quotes\"\n   - Prevents JSON issues\n   - Cleaner display\n\n**Alternative approach** (more robust):\n```typescript\n// Use output parser to enforce constraints\nimport { StringOutputParser } from 'langchain/schema/output_parser';\n\nconst parser = new StringOutputParser({\n  maxLength: 32,\n  sanitize: (text) => text.replace(/\"/g, ''),\n});\n\nconst title = await prompt.pipe(model).pipe(parser).invoke(input);\n```\n\n**Async vs Sync Title Generation:**\n\nCurrent: Async (happens after response sent)\n```typescript\n// Title generated while user reads response\n// Doesn't block chat experience\n```\n\nAlternative: Sync (blocking)\n```typescript\n// Wait for title before responding\n// Cleaner code, slower UX\n```\n\n**We choose**: Async for better UX!\n\n---\n**Next**: LLM model initialization!"
    },
    {
      "file": "packages/api/src/functions/chats-post.ts",
      "line": 62,
      "title": "LLM Model Configuration",
      "description": "## Initializing the Language Model ğŸ¤–\n\n### Azure OpenAI Configuration:\n```typescript\nconst model = new AzureChatOpenAI({\n  temperature: 0.7,  // Creativity parameter\n  azureADTokenProvider,\n});\n```\n\n### Local Ollama Configuration:\n```typescript\nconst model = new ChatOllama({\n  temperature: 0.7,\n  model: ollamaChatModel,  // \"llama3.1:latest\"\n});\n```\n\n---\n\n## Key Parameters:\n\n### 1. Temperature (0.0 - 2.0)\n\n**What it controls**: Randomness/creativity\n\n```\nTemperature 0.0:\n  \"The capital of France is Paris.\"\n  \"The capital of France is Paris.\"  â† Identical!\n  \nTemperature 0.7:\n  \"The capital of France is Paris.\"\n  \"Paris is the capital city of France.\"\n  \"France's capital is Paris.\"  â† Varied!\n  \nTemperature 2.0:\n  \"The capital of France is Paris.\"\n  \"Paris, the City of Light, is France's capital.\"\n  \"In France, you'll find Paris, the capital city.\"  â† Very creative!\n```\n\n**Our choice: 0.7**\n- **Too low** (0.0-0.3): Repetitive, boring, robotic\n- **Just right** (0.5-0.8): Natural, varied, helpful âœ…\n- **Too high** (1.0-2.0): Unpredictable, might go off-topic\n\n---\n\n### 2. Model Selection\n\n**Azure Options:**\n- **gpt-4o**: Best quality, slower, expensive\n- **gpt-4**: Great quality, moderate speed\n- **gpt-3.5-turbo**: Fast, cheap, good enough âœ…\n- **gpt-4o-mini**: Balanced, newer\n\n**Ollama Options:**\n- **llama3.1:70b**: Best quality, very slow\n- **llama3.1:8b**: Fast, good quality âœ…\n- **mistral**: Alternative, good for code\n- **phi3**: Smallest, fastest, lower quality\n\n**Trade-offs:**\n```\nQuality  â†‘     Cost â†‘     Speed â†“\n         GPT-4o\n              GPT-4\n                   GPT-3.5-turbo â† We use this\n                        GPT-4o-mini\n```\n\n---\n\n### 3. Other Parameters (Not Set, Use Defaults)\n\n**max_tokens**: Maximum response length\n```typescript\nmaxTokens: 800  // ~600 words max\n```\n- Default: Model's limit (4096 for GPT-3.5)\n- Lower = cheaper, faster, might cut off\n- Higher = more complete, but expensive\n\n**top_p**: Nucleus sampling (alternative to temperature)\n```typescript\ntopP: 0.9  // Consider top 90% probable tokens\n```\n- Usually not set when temperature is set\n- Controls diversity differently\n\n**frequency_penalty**: Reduce repetition\n```typescript\nfrequencyPenalty: 0.5  // Penalize repeated tokens\n```\n- 0.0 = no penalty\n- 2.0 = maximum penalty\n- Good for creative writing\n\n**presence_penalty**: Encourage new topics\n```typescript\npresencePenalty: 0.5  // Penalize tokens already used\n```\n- Pushes toward new topics\n- Good for diverse responses\n\n---\n\n### ğŸ’¡ Beginner: Model as a Knob\n\nThink of LLM configuration like audio settings:\n- **Temperature** = Creativity dial\n- **max_tokens** = Volume limiter\n- **Model** = Speaker quality\n\nStart with defaults, adjust only if needed!\n\n### ğŸ”¬ Advanced: Model Comparison\n\n**Benchmark (RAG Q&A task):**\n\n| Model | Latency | Cost/1K | Quality | RAM |\n|-------|---------|---------|---------|-----|\n| GPT-4o | 2-4s | $0.015 | 9.5/10 | N/A (API) |\n| GPT-3.5 | 1-2s | $0.002 | 8.5/10 | N/A (API) |\n| Llama3.1:70b | 10-30s | Free | 9/10 | 40GB+ |\n| Llama3.1:8b | 2-5s | Free | 8/10 | 8GB |\n\n**When to use which:**\n- **Learning/dev**: Llama3.1:8b (free!)\n- **Production (low cost)**: GPT-3.5-turbo\n- **Production (best quality)**: GPT-4o\n- **High volume**: Consider GPT-3.5 â†’ Cache results\n\n**Cost optimization:**\n```typescript\n// Cache common queries\nconst cache = new Map<string, string>();\n\nif (cache.has(question)) {\n  return cache.get(question);  // Free!\n}\n\nconst response = await model.invoke(...);\ncache.set(question, response);  // Cache for next time\n```\n\n---\n**Next**: Building the RAG chain!"
    },
    {
      "file": "packages/api/src/functions/chats-post.ts",
      "line": 87,
      "title": "Constructing the RAG Chain",
      "description": "## Building the Document Chain ğŸ”—\n\nThis is where context, query, and LLM come together!\n\n```typescript\nconst ragChain = await createStuffDocumentsChain({\n  llm: model,\n  prompt: ChatPromptTemplate.fromMessages([\n    ['system', ragSystemPrompt],\n    ['human', '{input}'],\n  ]),\n  documentPrompt: PromptTemplate.fromTemplate(\n    '[{source}]: {page_content}\\n'\n  ),\n});\n```\n\n---\n\n## What Each Part Does:\n\n### 1. `createStuffDocumentsChain`\n\n**\"Stuff\"** = Stuff all documents into the prompt!\n\n**How it works:**\n```typescript\n// Pseudo-code:\nfunction stuffDocuments(docs, prompt) {\n  let context = '';\n  for (const doc of docs) {\n    context += formatDocument(doc);  // Use documentPrompt\n  }\n  \n  const fullPrompt = prompt.replace('{context}', context);\n  return llm.invoke(fullPrompt);\n}\n```\n\n**Alternative chain types:**\n- **MapReduceDocumentsChain**: Process each doc separately, combine\n- **RefineDocumentsChain**: Iteratively refine answer\n- **MapRerankDocumentsChain**: Score each, pick best\n\n**We use \"Stuff\" because:**\n- âœ… Simple and reliable\n- âœ… Fast (single LLM call)\n- âœ… Works great for 3-5 documents\n- âŒ Doesn't scale to 100+ documents (token limits)\n\n---\n\n### 2. `ChatPromptTemplate`\n\n**Creates the conversation format:**\n```typescript\nChatPromptTemplate.fromMessages([\n  ['system', ragSystemPrompt],  // System instructions\n  ['human', '{input}'],         // User question\n])\n```\n\n**Becomes:**\n```\nSystem: Assistant helps Contoso Real Estate...\n        Answer ONLY with information from sources...\n        SOURCES:\n        [terms.pdf]: Pets: Up to 2 allowed...\n        [policy.pdf]: Pet deposits: $200...\n        \nHuman: What's the pet policy?\n```\n\n**LLM generates:**\n```\nAssistant: You can have up to 2 pets [terms.pdf]. \n           A $200 deposit per pet is required [policy.pdf].\n           \n           <<What breeds are restricted?>>\n           <<Is the deposit refundable?>>\n           <<Can I have a service animal?>>\n```\n\n---\n\n### 3. `documentPrompt`\n\n**Formats each retrieved document:**\n```typescript\nPromptTemplate.fromTemplate('[{source}]: {page_content}\\n')\n```\n\n**Input:**\n```javascript\n{\n  pageContent: \"Pets: Up to 2 animals allowed per unit...\",\n  metadata: { source: \"terms.pdf\", page: 5 }\n}\n```\n\n**Output:**\n```\n[terms.pdf]: Pets: Up to 2 animals allowed per unit...\n```\n\n**Why this format?**\n- Clear source indication for LLM\n- Easy to parse citations later\n- Human-readable in logs\n\n---\n\n### ğŸ’¡ Beginner: Chain Visualization\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Retrieved Documents (3)            â”‚\nâ”‚  1. [terms.pdf]: Pet info...        â”‚\nâ”‚  2. [policy.pdf]: Deposit info...   â”‚\nâ”‚  3. [guide.pdf]: Rules info...      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n               â”‚ documentPrompt formats each\n               â†“\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Context String                     â”‚\nâ”‚  [terms.pdf]: Pet info...           â”‚\nâ”‚  [policy.pdf]: Deposit info...      â”‚\nâ”‚  [guide.pdf]: Rules info...         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n               â”‚ Injected into {context}\n               â†“\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  System Prompt + Context            â”‚\nâ”‚  \"Answer only from sources...       â”‚\nâ”‚   SOURCES: [all formatted docs]\"    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n               â”‚\n               â†“\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  User Question                      â”‚\nâ”‚  \"What's the pet policy?\"           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n               â”‚ Combined\n               â†“\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  LLM                                â”‚\nâ”‚  Generates grounded response        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n               â†“\n         Final Answer\n```\n\n### ğŸ”¬ Advanced: Chain Optimization\n\n**Token counting:**\n```typescript\nimport { encoding_for_model } from 'tiktoken';\n\nconst enc = encoding_for_model('gpt-3.5-turbo');\nconst tokens = enc.encode(promptText);\nconsole.log(`Tokens used: ${tokens.length}`);\n\n// Optimize if > 3000 tokens:\n// - Fewer documents (K=2 instead of 3)\n// - Shorter system prompt\n// - Compress documents\n```\n\n**Caching:**\n```typescript\n// LangChain supports prompt caching\nconst chain = createStuffDocumentsChain({\n  llm: model.bind({ cache: true }),\n  // ...\n});\n// Repeated prompts reuse cached results!\n```\n\n---\n**Next**: Adding chat history!"
    },
    {
      "file": "packages/api/src/functions/chats-post.ts",
      "line": 100,
      "title": "Adding Chat History Support",
      "description": "## Wrapping with Chat History ğŸ’¬\n\nThis makes the assistant remember previous messages!\n\n```typescript\nconst ragChainWithHistory = new RunnableWithMessageHistory({\n  runnable: ragChain,           // The chain we just built\n  inputMessagesKey: 'input',    // Where current query goes\n  historyMessagesKey: 'chat_history',  // Where history goes\n  getMessageHistory: async () => chatHistory,  // Load history\n});\n```\n\n---\n\n## How Chat History Works:\n\n### Without History:\n```\nUser: \"What's the pet policy?\"\nBot: \"Up to 2 pets allowed.\"\n\nUser: \"What about deposits?\"  \nâŒ Bot: \"What deposits are you asking about?\"\n```\nNo context! Each question is isolated.\n\n### With History:\n```\nUser: \"What's the pet policy?\"\nBot: \"Up to 2 pets allowed.\"\n\nUser: \"What about deposits?\"  \nâœ… Bot: \"Pet deposits are $200 per pet.\"\n```\nBot understands \"deposits\" refers to pet deposits!\n\n---\n\n## The Magic:\n\n### History is injected into prompt:\n```typescript\nSystem: [RAG instructions + context]\n\nChat History:\nHuman: What's the pet policy?\nAssistant: Up to 2 pets allowed [terms.pdf].\n\nCurrent Question:\nHuman: What about deposits?\n```\n\n### LLM has full context:\n1. Knows we're discussing pets\n2. Can reference previous answer\n3. Maintains conversation flow\n\n---\n\n## Implementation Details:\n\n### Message Storage:\n\n**Azure (Cosmos DB):**\n```typescript\nconst chatHistory = new AzureCosmsosDBNoSQLChatMessageHistory({\n  sessionId,  // \"session-abc-123\"\n  userId,     // \"user-456\"\n  credentials,\n});\n```\n\n**Stored as:**\n```json\n{\n  \"sessionId\": \"session-abc-123\",\n  \"userId\": \"user-456\",\n  \"messages\": [\n    {\"role\": \"human\", \"content\": \"What's the pet policy?\"},\n    {\"role\": \"ai\", \"content\": \"Up to 2 pets allowed...\"}\n  ]\n}\n```\n\n**Local (File System):**\n```typescript\nconst chatHistory = new FileSystemChatMessageHistory({\n  sessionId,\n  userId,\n});\n```\n\n**Stored as:**\n```\n./.data/chat_history/\n  user-456_session-abc-123.json\n```\n\n---\n\n### Session Management:\n\n```typescript\nconst sessionId = chatContext?.sessionId || uuidv4();\n```\n\n**New session**: Generate UUID\n**Existing session**: Use provided ID\n\n**Session lifecycle:**\n1. User starts chat â†’ New sessionId\n2. Each message â†’ Same sessionId\n3. User clicks \"New Chat\" â†’ New sessionId\n4. Sessions persist (can resume later!)\n\n---\n\n### ğŸ’¡ Beginner: Why Sessions?\n\nImagine multiple conversations:\n```\nSession 1: Asking about pets\nSession 2: Asking about rent\nSession 3: Asking about maintenance\n```\n\nWithout sessions:\n- All mixed together\n- Can't resume conversations\n- Can't organize history\n\nWith sessions:\n- âœ… Each conversation separate\n- âœ… Can resume any conversation\n- âœ… Can delete/archive old chats\n\n### ğŸ”¬ Advanced: History Management\n\n**Token limits are a problem!**\n\nLong conversations â†’ Huge history â†’ Exceeds token limit\n\n**Solutions:**\n\n#### 1. Sliding Window (Simple)\n```typescript\n// Keep only last N messages\nconst messages = await chatHistory.getMessages();\nconst recentMessages = messages.slice(-10);  // Last 10 only\n```\n\n#### 2. Summarization (Smart)\n```typescript\n// Summarize old messages\nconst summary = await llm.invoke(\n  `Summarize this conversation: ${oldMessages}`\n);\n// Use summary + recent messages\n```\n\n#### 3. Selective Memory (Advanced)\n```typescript\n// Keep only relevant past messages\nconst relevantHistory = await retriever.invoke(\n  question, \n  chatHistory  // Search history for relevant exchanges\n);\n```\n\n**This sample**: Keeps full history (simple, works for short chats)\n\n**Production**: Should implement sliding window or summarization!\n\n---\n**Next**: Executing the chain with streaming!"
    },
    {
      "file": "packages/api/src/functions/chats-post.ts",
      "line": 112,
      "title": "Invoking the RAG Chain",
      "description": "## Generating the Response ğŸ¯\n\nThis is where everything comes together!\n\n```typescript\nconst question = messages.at(-1)!.content;\nconst responseStream = await ragChainWithHistory.stream(\n  {\n    input: question,\n    context: await retriever.invoke(question),\n  },\n  { configurable: { sessionId } }\n);\n```\n\n---\n\n## Step-by-Step Execution:\n\n### 1. Extract Current Question\n```typescript\nconst question = messages.at(-1)!.content;\n// \"What about deposits?\"\n```\n\n### 2. Retrieve Relevant Context\n```typescript\nconst context = await retriever.invoke(question);\n// Returns 3 most relevant chunks:\n// [\n//   { content: \"Pet deposits: $200 per pet...\", metadata: {...} },\n//   { content: \"Deposits are refundable...\", metadata: {...} },\n//   { content: \"Payment methods for deposits...\", metadata: {...} }\n// ]\n```\n\n### 3. Load Chat History\n```typescript\n// Automatically loaded by RunnableWithMessageHistory\nconst history = await chatHistory.getMessages();\n// [\n//   {role: \"human\", content: \"What's the pet policy?\"},\n//   {role: \"ai\", content: \"Up to 2 pets allowed...\"}\n// ]\n```\n\n### 4. Build Complete Prompt\n```typescript\n// LangChain combines:\n// - System prompt (with context)\n// - Chat history  \n// - Current question\n\nconst fullPrompt = `\nSystem: Assistant helps Contoso Real Estate...\n        Answer ONLY from sources below...\n        SOURCES:\n        [terms.pdf]: Pet deposits: $200 per pet...\n        [policy.pdf]: Deposits are refundable...\n        [guide.pdf]: Payment methods for deposits...\n        \nChat History:\nHuman: What's the pet policy?\nAssistant: Up to 2 pets allowed [terms.pdf].\n\nHuman: What about deposits?\n`;\n```\n\n### 5. Stream from LLM\n```typescript\n// Model generates response token by token:\n// \"Pet\" â†’ \"deposits\" â†’ \"are\" â†’ \"$200\" â†’ ...\n\nfor await (const chunk of responseStream) {\n  console.log(chunk);  // Stream each token\n}\n```\n\n### 6. Final Response\n```\nPet deposits are $200 per pet [terms.pdf] and are fully \nrefundable at the end of your lease [policy.pdf]. You can \npay deposits via credit card or bank transfer [guide.pdf].\n\n<<Are there any non-refundable fees?>>\n<<When is the deposit refunded?>>\n<<Can I pay in installments?>>\n```\n\n---\n\n## The Flow Diagram:\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Question   â”‚ \"What about deposits?\"\nâ””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n       â”‚\n       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n       â”‚             â”‚\n       â†“             â†“\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Retrieval â”‚  â”‚ History  â”‚\nâ”‚  (context) â”‚  â”‚  Load    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n       â”‚              â”‚\n       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n              â†“\n       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n       â”‚ Build Prompt â”‚\n       â”‚ System + CTX â”‚\n       â”‚ + History +  â”‚\n       â”‚   Question   â”‚\n       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n              â†“\n       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n       â”‚     LLM      â”‚\n       â”‚  Generates   â”‚\n       â”‚   Response   â”‚\n       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n              â†“\n       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n       â”‚    Stream    â”‚\n       â”‚  to Client   â”‚\n       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n---\n\n### ğŸ’¡ Beginner: The Key Insight\n\n**RAG = Context + History + Question â†’ LLM**\n\n1. **Context** (from retrieval): \"What facts are available?\"\n2. **History** (from chat): \"What have we discussed?\"\n3. **Question** (from user): \"What do they want to know?\"\n\nLLM combines all three to generate a grounded, contextual answer!\n\n### ğŸ”¬ Advanced: Execution Optimizations\n\n**Parallel Retrieval + History Load:**\n```typescript\n// Current: Sequential\nconst context = await retriever.invoke(question);\nconst history = await chatHistory.getMessages();\n\n// Optimized: Parallel\nconst [context, history] = await Promise.all([\n  retriever.invoke(question),\n  chatHistory.getMessages(),\n]);\n// Saves 10-50ms!\n```\n\n**Streaming Benefits:**\n- First token in ~500ms\n- vs full response in 3-5s\n- 5-10x better perceived latency!\n\n**Error Handling:**\n```typescript\ntry {\n  const responseStream = await ragChainWithHistory.stream(...);\n  // ...\n} catch (error) {\n  if (error.code === 'context_length_exceeded') {\n    // Retry with fewer context documents\n    return retryWithFewerDocs();\n  }\n  throw error;\n}\n```\n\n---\n**Next**: Tour complete!"
    },
    {
      "title": "Response Generation Tour Complete! ğŸ‰",
      "description": "## Tour 5 Complete - Generation Mastered! âœ…\n\n### What You've Learned:\n\n#### Core Concepts:\n\n**Beginner Level:**\nâœ… System prompts instruct LLM behavior\nâœ… Retrieved context prevents hallucinations\nâœ… Chat history enables multi-turn conversations\nâœ… LangChain chains orchestrate the pipeline\nâœ… Streaming delivers real-time responses\n\n**Advanced Level:**\nâœ… Prompt engineering techniques (instructions, examples, CoT)\nâœ… Temperature and model parameter tuning\nâœ… Chain composition patterns (stuff, map-reduce, refine)\nâœ… Token management and cost optimization\nâœ… Session management and history strategies\n\n---\n\n## ğŸ¯ The Complete RAG Chain:\n\n```\nğŸ“ System Prompt (Instructions)\n      â†“\nğŸ“‹ Retrieved Context (Facts)\n      â†“  \nğŸ’¬ Chat History (Memory)\n      â†“\nğŸ‘¤ User Question (Query)\n      â†“\nğŸ¤– LLM (Processing)\n      â†“\nğŸ’¬ Response + Citations\n      â†“\nğŸ“¡ Stream to User\n```\n\n---\n\n## ğŸ’¡ Key Takeaways:\n\n### Prompt = Programming the LLM\n\n**Good prompts:**\n- Explicit instructions\n- Clear constraints  \n- Format examples\n- Error handling\n\n**Poor prompts:**\n- Vague instructions\n- Implicit assumptions\n- No examples\n- Missing edge cases\n\n### The RAG Triangle:\n\n```\n       Context\n         â†— â†–\n        /   \\  \n       /     \\\nHistory â† â†’ Question\n```\n\nAll three are needed for accurate, contextual responses!\n\n---\n\n## ğŸš€ What's Next?\n\nOne more tour to complete the RAG journey!\n\n### [Tour 6: Streaming & Chat History]\nExplore:\n- How HTTP streaming works\n- Real-time token delivery\n- Chat history persistence\n- Session management\n- Frontend integration\n\n---\n\n## ğŸ§ª Experiment Ideas:\n\n1. **Test temperature**:\n   - Try 0.0, 0.5, 1.0, 2.0\n   - See how responses change\n\n2. **Modify system prompt**:\n   - Remove \"Answer ONLY from sources\"\n   - See if LLM hallucinates\n   - Add back, see difference\n\n3. **Test without history**:\n   - Ask follow-up questions\n   - See how context is lost\n\n4. **Compare models**:\n   - GPT-3.5 vs GPT-4\n   - Llama 3.1 8b vs 70b\n   - Measure quality differences\n\n5. **Token counting**:\n   - Install tiktoken\n   - Count tokens in prompts\n   - Calculate costs\n\n---\n\n### Continue Learning:\n[Tour 6: Streaming & Chat History â†’]\n\n---\n\n**Further Reading:**\n- [LangChain Chains](https://js.langchain.com/docs/modules/chains/)\n- [Prompt Engineering Guide](https://www.promptingguide.ai/)\n- [OpenAI Best Practices](https://platform.openai.com/docs/guides/prompt-engineering)\n- [LLM Parameters Explained](https://huggingface.co/blog/how-to-generate)"
    }
  ]
}
