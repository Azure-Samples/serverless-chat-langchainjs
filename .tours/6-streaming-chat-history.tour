{
  "$schema": "https://aka.ms/codetour-schema",
  "title": "6 - Streaming & Chat History: Real-Time UX",
  "description": "Final tour! Learn how responses are streamed in real-time to users and how chat sessions are persisted for multi-turn conversations.",
  "steps": [
    {
      "title": "Streaming & History Overview",
      "description": "## Welcome to Tour 6: Streaming & Chat History! ğŸ“¡\n\n### What We'll Cover:\n1. Why streaming matters for UX\n2. How HTTP streaming works in Azure Functions\n3. Streaming response format (NDJSON)\n4. Chat history persistence\n5. Session management across requests\n6. Frontend integration\n\n### The Complete Request/Response Flow:\n```\nğŸ‘¤ User types message in browser\n      â†“\nğŸŒ HTTP POST to /chats/stream\n      â†“\nğŸ” RAG Pipeline (retrieval + generation)\n      â†“\nğŸ“¡ Stream tokens as they're generated\n      â†“\nğŸ’¬ User sees response appear in real-time\n      â†“\nğŸ’¾ Save message + response to chat history\n```\n\n### Why This Matters:\n- **Streaming** = Better UX (feels instant!)\n- **Chat history** = Multi-turn conversations\n- **Sessions** = User can resume chats\n\n---\n\n### ğŸ’¡ Beginner: Non-Streaming vs Streaming\n\n**Without Streaming:**\n```\nUser: \"Explain RAG\"\n[5 second wait...]\nBot: [Full response appears all at once]\n```\n\n**With Streaming:**\n```\nUser: \"Explain RAG\"\n[500ms]\nBot: \"RAG\"\nBot: \"RAG stands for\"\nBot: \"RAG stands for Retrieval\"\nBot: \"RAG stands for Retrieval-Augmented\"\nBot: \"RAG stands for Retrieval-Augmented Generation...\"\n```\n\nSame total time, but feels 10x faster!\n\n### ğŸ”¬ Advanced Preview:\n- HTTP/1.1 chunked transfer encoding\n- Server-Sent Events (SSE) alternative\n- Newline-delimited JSON (NDJSON)\n- Backpressure handling\n\n---\n**Let's explore streaming!**"
    },
    {
      "file": "packages/api/src/functions/chats-post.ts",
      "line": 158,
      "title": "Enabling HTTP Streaming",
      "description": "## Azure Functions HTTP Streaming Setup ğŸ”§\n\nThis enables streaming responses:\n\n```typescript\napp.setup({ enableHttpStream: true });\napp.http('chats-post', {\n  route: 'chats/stream',\n  methods: ['POST'],\n  authLevel: 'anonymous',\n  handler: postChats,\n});\n```\n\n---\n\n## What `enableHttpStream` Does:\n\n### Without Streaming (Default):\n```typescript\n// Function must return complete response\nreturn {\n  status: 200,\n  body: fullResponseString  // Wait for entire response\n};\n```\n\n**Process:**\n1. Generate full response (3-5s)\n2. Return complete string\n3. Client receives all at once\n\n### With Streaming:\n```typescript\n// Function can return a stream\nreturn {\n  status: 200,\n  body: readableStream  // Send chunks as available\n};\n```\n\n**Process:**\n1. Generate first token (500ms)\n2. Send immediately\n3. Generate next token\n4. Send immediately\n5. Repeat until done\n\n---\n\n## HTTP/1.1 Chunked Transfer Encoding:\n\n**How it works:**\n```\nHTTP/1.1 200 OK\nTransfer-Encoding: chunked\nContent-Type: application/x-ndjson\n\n14\\r\\n  â† Chunk size in hex (20 bytes)\n{\"delta\":{\"content\":\"Hello\"}}\\r\\n  â† Chunk data\n15\\r\\n  â† Next chunk size\n{\"delta\":{\"content\":\" world\"}}\\r\\n\n0\\r\\n  â† Size 0 = end\n\\r\\n\n```\n\n**Browser receives chunks as they arrive!**\n\n---\n\n### ğŸ’¡ Beginner: Why This Works\n\nNormal HTTP:\n- Server: \"Wait, I'm preparing your response...\"\n- [5 seconds pass]\n- Server: \"OK, here's everything!\"\n- Client: [Receives all at once]\n\nStreaming HTTP:\n- Server: \"Here's the first part...\"\n- Client: [Displays immediately]\n- Server: \"Here's more...\"\n- Client: [Displays immediately]\n- [Repeat until done]\n\n**Result**: User sees progress in real-time!\n\n### ğŸ”¬ Advanced: Streaming Protocols\n\n**Chunked Transfer Encoding** (What we use):\n- âœ… Simple, standard HTTP/1.1\n- âœ… Works with any client\n- âŒ No automatic reconnection\n- âŒ Unidirectional (server â†’ client)\n\n**Server-Sent Events (SSE):**\n```typescript\nContent-Type: text/event-stream\n\ndata: {\"content\": \"Hello\"}\n\ndata: {\"content\": \" world\"}\n\n```\n- âœ… Built-in reconnection\n- âœ… Event IDs for replay\n- âŒ Text-only (must encode JSON)\n\n**WebSockets:**\n- âœ… Bidirectional\n- âœ… Low latency\n- âŒ More complex setup\n- âŒ Stateful (harder to scale)\n\n**For RAG, chunked encoding is perfect!**\n\n---\n**Next**: See how responses are streamed!"
    },
    {
      "file": "packages/api/src/functions/chats-post.ts",
      "line": 112,
      "title": "Streaming the RAG Response",
      "description": "## Generating the Stream ğŸŒŠ\n\n```typescript\nconst responseStream = await ragChainWithHistory.stream(\n  {\n    input: question,\n    context: await retriever.invoke(question),\n  },\n  { configurable: { sessionId } }\n);\n```\n\n---\n\n## How LLM Streaming Works:\n\n### LLM Generation Process:\n\n**Step 1**: LLM predicts next token\n```\nPrompt: \"The capital of France is\"\nPrediction: \"Paris\" (probability: 0.92)\n```\n\n**Step 2**: Generate token, send immediately\n```javascript\nyield \"Paris\"  // Stream this token\n```\n\n**Step 3**: Append to context, predict next\n```\nPrompt: \"The capital of France is Paris\"\nPrediction: \".\" (probability: 0.87)\n```\n\n**Step 4**: Continue until done\n```javascript\nyield \".\"  // Stream this token\n// [End of generation]\n```\n\n### What `.stream()` Returns:\n\n```typescript\nAsyncIterator<string>\n\n// Usage:\nfor await (const chunk of responseStream) {\n  console.log(chunk);  // Each token as it's generated\n}\n\n// Example output:\n// \"Pet\"\n// \" deposits\"\n// \" are\"\n// \" $\"\n// \"200\"\n// ...\n```\n\n---\n\n## Performance Comparison:\n\n### Non-Streaming (`.invoke()`):\n```\nTime â†’  0s    1s    2s    3s    4s    5s\n        |-----|-----|-----|-----|-----|  \nLLM:    [Generating..................]  \nUser:   [Waiting....................]  \n                                     â†“\n                              Full response\n```\n**Time to first content**: 5s\n**Total time**: 5s\n\n### Streaming (`.stream()`):\n```\nTime â†’  0s    1s    2s    3s    4s    5s\n        |-----|-----|-----|-----|-----|  \nLLM:    [Generating token by token..]\nUser:   â†“     â†“     â†“     â†“     â†“     â†“\n        \"Pet\" \" deposits\" \" are\" \"$200\" ...\n```\n**Time to first content**: 500ms âœ¨\n**Total time**: Still 5s (same)\n\n**Perceived performance**: 10x better!\n\n---\n\n### ğŸ’¡ Beginner: Token vs Word\n\n**Token** â‰  Word (usually)\n\n```\n\"Pet deposits are $200\"\n\nTokens: [\"Pet\", \" deposits\", \" are\", \" $\", \"200\"]\n        â†‘      â†‘          â†‘      â†‘    â†‘\n        5 tokens for 4 words\n```\n\n**Why?**\n- Space is part of token\n- Numbers often separate\n- Punctuation separate\n- Subword tokenization\n\n**Average**: 1 token â‰ˆ 0.75 words\n\n### ğŸ”¬ Advanced: Streaming Internals\n\n**LangChain's `.stream()` implementation:**\n\n```typescript\nasync *stream(input) {\n  // 1. Build prompt\n  const prompt = await this.formatPrompt(input);\n  \n  // 2. Stream from LLM\n  for await (const chunk of this.llm.stream(prompt)) {\n    // 3. Post-process if needed\n    const processed = this.processChunk(chunk);\n    \n    // 4. Yield to caller\n    yield processed;\n  }\n}\n```\n\n**Benefits of async generators:**\n- Natural backpressure handling\n- Memory efficient (no buffering)\n- Composable (can chain streams)\n- Cancel-able (break to stop)\n\n**Example - Cancellation:**\n```typescript\nconst controller = new AbortController();\n\n// User can cancel generation\nfor await (const chunk of stream) {\n  if (controller.signal.aborted) {\n    break;  // Stop generating\n  }\n  displayChunk(chunk);\n}\n```\n\n---\n**Next**: Converting to JSON stream!"
    },
    {
      "file": "packages/api/src/functions/chats-post.ts",
      "line": 147,
      "title": "JSON Stream Formatting",
      "description": "## Converting to NDJSON Format ğŸ“„\n\n```typescript\nasync function* createJsonStream(\n  chunks: AsyncIterable<string>, \n  sessionId: string\n) {\n  for await (const chunk of chunks) {\n    if (!chunk) continue;\n\n    const responseChunk: AIChatCompletionDelta = {\n      delta: {\n        content: chunk,\n        role: 'assistant',\n      },\n      context: {\n        sessionId,\n      },\n    };\n\n    // Format as Newline-delimited JSON\n    yield JSON.stringify(responseChunk) + '\\n';\n  }\n}\n```\n\n---\n\n## NDJSON Format:\n\n**NDJSON** = Newline-Delimited JSON\n\n### Why Not Regular JSON?\n\n**Regular JSON (doesn't work for streaming):**\n```json\n[\n  {\"delta\": {\"content\": \"Hello\"}},\n  {\"delta\": {\"content\": \" world\"}}\n]\n```\nâŒ Can't parse until complete (need closing `]`)\n\n**NDJSON (perfect for streaming):**\n```json\n{\"delta\": {\"content\": \"Hello\"}}\n{\"delta\": {\"content\": \" world\"}}\n```\nâœ… Each line is valid JSON\nâœ… Parse line-by-line as they arrive!\n\n---\n\n## Stream Format Example:\n\n### Raw LLM Output:\n```\n\"Pet\" â†’ \" deposits\" â†’ \" are\" â†’ \" $200\"\n```\n\n### After `createJsonStream`:\n```json\n{\"delta\":{\"content\":\"Pet\",\"role\":\"assistant\"},\"context\":{\"sessionId\":\"abc-123\"}}\n{\"delta\":{\"content\":\" deposits\",\"role\":\"assistant\"},\"context\":{\"sessionId\":\"abc-123\"}}\n{\"delta\":{\"content\":\" are\",\"role\":\"assistant\"},\"context\":{\"sessionId\":\"abc-123\"}}\n{\"delta\":{\"content\":\" $200\",\"role\":\"assistant\"},\"context\":{\"sessionId\":\"abc-123\"}}\n```\n\n### Client Receives:\n```typescript\n// Line 1\nconst chunk1 = JSON.parse(line1);\nconsole.log(chunk1.delta.content);  // \"Pet\"\n\n// Line 2\nconst chunk2 = JSON.parse(line2);\nconsole.log(chunk2.delta.content);  // \" deposits\"\n\n// Concatenate:\nlet fullResponse = chunk1.delta.content + chunk2.delta.content;\n// \"Pet deposits\"\n```\n\n---\n\n## The `AIChatCompletionDelta` Type:\n\n```typescript\ninterface AIChatCompletionDelta {\n  delta: {\n    content: string;      // The actual text token\n    role: 'assistant';    // Who's speaking\n  };\n  context: {\n    sessionId: string;    // Which conversation\n  };\n}\n```\n\n**Why this structure?**\n\n1. **`delta`**: Indicates incremental content\n   - vs `message`: Complete message\n\n2. **`role`**: Maintains conversation structure\n   - Follows OpenAI API format\n\n3. **`context`**: Metadata about the response\n   - Session tracking\n   - Could include: usage stats, model info, etc.\n\n---\n\n### ğŸ’¡ Beginner: Why JSON?\n\n**JSON is universal:**\n- âœ… Every language can parse it\n- âœ… Type-safe (know what to expect)\n- âœ… Extensible (add fields without breaking)\n- âœ… Human-readable (easy debugging)\n\n**Alternative formats:**\n- Plain text: Hard to add metadata\n- Binary: Efficient but complex\n- XML: Verbose, outdated\n\n**JSON + NDJSON = Perfect for streaming APIs!**\n\n### ğŸ”¬ Advanced: Stream Protocol Standards\n\n**OpenAI Chat Completions API** (what we follow):\n```json\n{\"choices\":[{\"delta\":{\"content\":\"Hello\"},\"index\":0}]}\n```\n\n**Anthropic Claude API**:\n```json\n{\"type\":\"content_block_delta\",\"delta\":{\"text\":\"Hello\"}}\n```\n\n**Our implementation**:\n```json\n{\"delta\":{\"content\":\"Hello\",\"role\":\"assistant\"}}\n```\n\n**Why compatible?**\n- Easy migration between providers\n- Frontend libraries expect this format\n- Industry standard emerging\n\n**See**: [AI Chat Protocol](https://aka.ms/chatprotocol)\n\n---\n**Next**: Returning the stream response!"
    },
    {
      "file": "packages/api/src/functions/chats-post.ts",
      "line": 120,
      "title": "Returning the Stream Response",
      "description": "## Sending the Stream to Client ğŸ“¤\n\n```typescript\nconst jsonStream = Readable.from(\n  createJsonStream(responseStream, sessionId)\n);\n\nreturn data(jsonStream, {\n  'Content-Type': 'application/x-ndjson',\n  'Transfer-Encoding': 'chunked',\n});\n```\n\n---\n\n## Breaking This Down:\n\n### 1. Convert to Node.js Stream\n\n```typescript\nReadable.from(createJsonStream(responseStream, sessionId))\n```\n\n**Why?**\n- Azure Functions expects Node.js `Readable` stream\n- `createJsonStream` returns async generator\n- `Readable.from()` converts generator â†’ stream\n\n**Conceptually:**\n```typescript\nAsync Generator â†’ Node.js Stream â†’ HTTP Response\n```\n\n---\n\n### 2. Set Response Headers\n\n```typescript\n{\n  'Content-Type': 'application/x-ndjson',\n  'Transfer-Encoding': 'chunked',\n}\n```\n\n#### `Content-Type: application/x-ndjson`\n\n**Tells client**: \"This is NDJSON format\"\n\n**Client knows to:**\n- Split by newlines\n- Parse each line as JSON\n- Handle incrementally\n\n#### `Transfer-Encoding: chunked`\n\n**Tells client**: \"Response will come in chunks\"\n\n**Browser knows to:**\n- Don't wait for Content-Length\n- Process chunks as they arrive\n- Stream is done when size = 0\n\n---\n\n## The `data()` Helper:\n\n```typescript\n// From http-response.ts\nexport function data(\n  body: Readable,\n  headers: Record<string, string>\n): HttpResponseInit {\n  return {\n    status: 200,\n    body,\n    headers,\n  };\n}\n```\n\n**Simple wrapper** for consistent response format!\n\n---\n\n## Complete HTTP Response:\n\n```http\nHTTP/1.1 200 OK\nContent-Type: application/x-ndjson\nTransfer-Encoding: chunked\n\n{\"delta\":{\"content\":\"Pet\",\"role\":\"assistant\"},\"context\":{\"sessionId\":\"abc-123\"}}\n{\"delta\":{\"content\":\" deposits\",\"role\":\"assistant\"},\"context\":{\"sessionId\":\"abc-123\"}}\n{\"delta\":{\"content\":\" are\",\"role\":\"assistant\"},\"context\":{\"sessionId\":\"abc-123\"}}\n{\"delta\":{\"content\":\" $200\",\"role\":\"assistant\"},\"context\":{\"sessionId\":\"abc-123\"}}\n```\n\n---\n\n### ğŸ’¡ Beginner: The Journey of a Token\n\n```\n1. LLM generates: \"Pet\"\n   â†“\n2. LangChain yields: \"Pet\"\n   â†“\n3. createJsonStream wraps: {\"delta\":{\"content\":\"Pet\"}}\n   â†“\n4. Readable.from sends to HTTP stream\n   â†“\n5. Azure Functions sends HTTP chunk\n   â†“\n6. Browser receives chunk\n   â†“\n7. Frontend parses JSON\n   â†“\n8. User sees: \"Pet\"\n```\n\n**Time**: ~10-50ms per token!\n\n### ğŸ”¬ Advanced: Streaming Performance\n\n**Metrics to monitor:**\n\n1. **Time to First Token (TTFT)**\n   - Retrieval + LLM start\n   - Target: < 500ms\n\n2. **Tokens Per Second (TPS)**\n   - LLM generation speed\n   - GPT-3.5: ~40-60 TPS\n   - GPT-4: ~20-30 TPS\n   - Llama 3.1 (CPU): ~5-10 TPS\n   - Llama 3.1 (GPU): ~30-50 TPS\n\n3. **Total Time**\n   - TTFT + (tokens / TPS)\n   - Example: 500ms + (100 tokens / 50 TPS) = 2.5s\n\n**Optimization strategies:**\n\n```typescript\n// 1. Parallel retrieval + model warm-up\nawait Promise.all([\n  retriever.invoke(question),\n  model.invoke(\"warmup\"),  // Pre-load model\n]);\n\n// 2. Speculative decoding (advanced)\n// Generate multiple tokens per step\n// 2-3x faster for CPU models\n\n// 3. Caching\nif (cache.has(question)) {\n  return cache.get(question);  // Instant!\n}\n```\n\n---\n**Next**: Chat history persistence!"
    },
    {
      "file": "packages/api/src/functions/chats-post.ts",
      "line": 124,
      "title": "Chat History Management",
      "description": "## Saving Chat History ğŸ’¾\n\nAfter streaming the response, we save the chat title:\n\n```typescript\nconst { title } = await chatHistory.getContext();\nif (!title) {\n  const response = await ChatPromptTemplate\n    .fromMessages([\n      ['system', titleSystemPrompt],\n      ['human', '{input}'],\n    ])\n    .pipe(model)\n    .invoke({ input: question });\n  \n  context.log(`Title for session: ${response.content}`);\n  chatHistory.setContext({ title: response.content });\n}\n```\n\n---\n\n## How Chat History Works:\n\n### 1. Message Storage\n\n**Each message is saved:**\n```typescript\n// Automatically saved by RunnableWithMessageHistory\nconst ragChainWithHistory = new RunnableWithMessageHistory({\n  runnable: ragChain,\n  getMessageHistory: async () => chatHistory,\n});\n\n// When invoked:\nawait ragChainWithHistory.stream({ input: question });\n\n// Both saved:\n// - User message: question\n// - Assistant message: generated response\n```\n\n### 2. Storage Backends\n\n**Azure Cosmos DB:**\n```typescript\nconst chatHistory = new AzureCosmsosDBNoSQLChatMessageHistory({\n  sessionId,\n  userId,\n  credentials,\n});\n```\n\n**Stored as:**\n```json\n{\n  \"id\": \"session-abc-123\",\n  \"userId\": \"user-456\",\n  \"context\": {\n    \"title\": \"Pet policy inquiry\"\n  },\n  \"messages\": [\n    {\n      \"type\": \"human\",\n      \"data\": {\"content\": \"What's the pet policy?\"},\n      \"timestamp\": \"2024-01-15T10:30:00Z\"\n    },\n    {\n      \"type\": \"ai\",\n      \"data\": {\"content\": \"You can have up to 2 pets...\"},\n      \"timestamp\": \"2024-01-15T10:30:05Z\"\n    }\n  ]\n}\n```\n\n**Local File System:**\n```typescript\nconst chatHistory = new FileSystemChatMessageHistory({\n  sessionId,\n  userId,\n});\n```\n\n**Stored as:**\n```\n./.data/chat_history/\n  user-456_session-abc-123.json\n```\n\n---\n\n## Session Lifecycle:\n\n### First Message:\n```typescript\n// 1. Generate new sessionId\nconst sessionId = uuidv4();  // \"session-abc-123\"\n\n// 2. Send with response\nreturn { sessionId, messages: [...] };\n\n// 3. Client stores sessionId\nlocalStorage.setItem('sessionId', sessionId);\n```\n\n### Subsequent Messages:\n```typescript\n// 1. Client sends existing sessionId\nPOST /chats/stream\n{\n  messages: [...],\n  context: { sessionId: \"session-abc-123\" }\n}\n\n// 2. Server loads history\nconst history = await chatHistory.getMessages();\n\n// 3. Continues conversation\n```\n\n### New Chat:\n```typescript\n// 1. User clicks \"New Chat\"\n// 2. Client generates new sessionId\nconst newSessionId = uuidv4();\n\n// 3. Starts fresh conversation\n```\n\n---\n\n### ğŸ’¡ Beginner: Why Sessions?\n\n**Without sessions:**\n```\nUser: \"What's the pet policy?\"\nBot: \"Up to 2 pets.\"\n[User closes browser]\n[User returns next day]\nUser: \"What did we discuss?\"\nBot: \"I don't know.\"  âŒ Lost!\n```\n\n**With sessions:**\n```\nUser: \"What's the pet policy?\"\nBot: \"Up to 2 pets.\"\n[User closes browser]\n[User returns next day]\n[Bot loads session]\nUser: \"What did we discuss?\"\nBot: \"We talked about pet policies.\"  âœ… Remembered!\n```\n\n### ğŸ”¬ Advanced: History Management Strategies\n\n**1. Sliding Window (Token Limit)**\n```typescript\nclass SlidingWindowHistory {\n  async getMessages() {\n    const all = await this.storage.getAll();\n    \n    // Count tokens, keep only recent messages\n    let tokens = 0;\n    const kept = [];\n    for (const msg of all.reverse()) {\n      tokens += countTokens(msg);\n      if (tokens > 2000) break;\n      kept.unshift(msg);\n    }\n    return kept;\n  }\n}\n```\n\n**2. Summarization (Compress Old)**\n```typescript\nif (messages.length > 20) {\n  const oldMessages = messages.slice(0, 10);\n  const summary = await llm.invoke(\n    `Summarize: ${oldMessages}`\n  );\n  \n  // Replace old messages with summary\n  return [summary, ...messages.slice(10)];\n}\n```\n\n**3. Hierarchical (Store All, Load Relevant)**\n```typescript\n// Store everything\nawait storage.save(message);\n\n// Load only relevant to current query\nconst relevant = await vectorSearch(\n  currentQuestion,\n  allHistoryMessages\n);\n```\n\n**Trade-offs:**\n- **Sliding Window**: Simple, may lose context\n- **Summarization**: Preserves key info, costs LLM calls\n- **Hierarchical**: Best quality, most complex\n\n---\n**Next**: Frontend integration!"
    },
    {
      "file": "packages/webapp/src/api.ts",
      "line": 1,
      "title": "Frontend API Client",
      "description": "## Client-Side Integration ğŸŒ\n\nThe frontend consumes the streaming API:\n\n```typescript\nexport async function* getCompletion(\n  options: ChatRequestOptions\n): AsyncGenerator<AIChatCompletionDelta> {\n  const response = await fetch(url, {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify(request),\n  });\n\n  const reader = response.body!.getReader();\n  const decoder = new TextDecoder();\n\n  while (true) {\n    const { done, value } = await reader.read();\n    if (done) break;\n\n    const text = decoder.decode(value);\n    const lines = text.split('\\n').filter(Boolean);\n\n    for (const line of lines) {\n      yield JSON.parse(line) as AIChatCompletionDelta;\n    }\n  }\n}\n```\n\n---\n\n## How the Client Works:\n\n### 1. Fetch API (Streaming)\n\n```typescript\nconst response = await fetch(url, { method: 'POST', ... });\n```\n\n**Returns** `Response` with `body: ReadableStream`\n\n### 2. Stream Reader\n\n```typescript\nconst reader = response.body.getReader();\n```\n\n**Provides** `.read()` method to consume chunks\n\n### 3. Reading Chunks\n\n```typescript\nwhile (true) {\n  const { done, value } = await reader.read();\n  if (done) break;\n  // value is Uint8Array (raw bytes)\n}\n```\n\n### 4. Decode Bytes â†’ Text\n\n```typescript\nconst decoder = new TextDecoder();\nconst text = decoder.decode(value);\n// Uint8Array â†’ String\n```\n\n### 5. Split by Newlines (NDJSON)\n\n```typescript\nconst lines = text.split('\\n').filter(Boolean);\n// ['{\"delta\":...}', '{\"delta\":...}']\n```\n\n### 6. Parse Each Line\n\n```typescript\nfor (const line of lines) {\n  yield JSON.parse(line);  // Return to caller\n}\n```\n\n---\n\n## Usage in UI Component:\n\n```typescript\n// From chat.ts component\nasync onSendClicked() {\n  const response = getCompletion({\n    messages: this.messages,\n    context: { sessionId: this.sessionId }\n  });\n\n  let fullResponse = '';\n  \n  for await (const chunk of response) {\n    fullResponse += chunk.delta.content;\n    \n    // Update UI immediately!\n    this.messages[lastIndex].content = fullResponse;\n    this.scrollToLastMessage();\n  }\n}\n```\n\n**User sees:**\n```\n\"Pet\" â†’ \"Pet deposits\" â†’ \"Pet deposits are\" â†’ \"Pet deposits are $200\"\n```\n\n**Each update** renders immediately!\n\n---\n\n### ğŸ’¡ Beginner: Browser Streams\n\n**Modern browsers support streaming natively:**\n\n```javascript\nfetch(url)\n  .then(response => response.body.getReader())\n  .then(reader => {\n    // Read chunks as they arrive\n    return reader.read();\n  });\n```\n\n**No special libraries needed!**\n\n**Old approach (before streams):**\n```javascript\n// XMLHttpRequest with progress events\nxhr.onprogress = (e) => {\n  // Can see bytes but not parse incrementally\n};\n```\n\n### ğŸ”¬ Advanced: Stream Considerations\n\n**1. Buffering Issues:**\n```typescript\n// Problem: Chunks might split mid-JSON\nconst text = decoder.decode(value);\n// Result: '{\"delta\":{\"con' (incomplete!)\n\n// Solution: Buffer incomplete lines\nlet buffer = '';\nfor (const { value } of reader) {\n  buffer += decoder.decode(value);\n  const lines = buffer.split('\\n');\n  buffer = lines.pop() || '';  // Keep incomplete line\n  \n  for (const line of lines.filter(Boolean)) {\n    yield JSON.parse(line);\n  }\n}\n```\n\n**2. Error Handling:**\n```typescript\ntry {\n  for await (const chunk of stream) {\n    displayChunk(chunk);\n  }\n} catch (error) {\n  if (error.name === 'AbortError') {\n    // User cancelled\n  } else {\n    // Network error, show partial response\n    showError('Connection lost', partialResponse);\n  }\n}\n```\n\n**3. Cancellation:**\n```typescript\nconst controller = new AbortController();\n\nfetch(url, { signal: controller.signal });\n\n// User can cancel:\nstopButton.onclick = () => {\n  controller.abort();  // Stops stream\n};\n```\n\n---\n**Next**: Tour complete!"
    },
    {
      "title": "Complete RAG Journey Finished! ğŸ‰",
      "description": "## Congratulations! All Tours Complete! ğŸ†\n\n### ğŸ¯ You've Mastered the Complete RAG Pipeline!\n\n#### Tour 1: RAG Overview\nâœ… Understanding RAG architecture\nâœ… Three phases: Ingestion, Retrieval, Generation\nâœ… Why RAG prevents hallucinations\n\n#### Tour 2: Document Ingestion\nâœ… PDF loading and text extraction\nâœ… Chunking strategies and overlap\nâœ… Embedding generation\nâœ… Vector storage (Cosmos DB & FAISS)\n\n#### Tour 3: Vector Storage\nâœ… Vector database internals\nâœ… Similarity metrics (cosine, euclidean)\nâœ… Indexing algorithms (DiskANN, HNSW)\nâœ… Storage optimization techniques\n\n#### Tour 4: Query & Retrieval\nâœ… Query embedding process\nâœ… Vector similarity search\nâœ… Top-K selection strategies\nâœ… Retrieval evaluation metrics\n\n#### Tour 5: Response Generation\nâœ… Prompt engineering for RAG\nâœ… LangChain chain composition\nâœ… LLM configuration and parameters\nâœ… Chat history integration\n\n#### Tour 6: Streaming & History (This Tour!)\nâœ… HTTP streaming protocols\nâœ… NDJSON format\nâœ… Real-time token delivery\nâœ… Session management\nâœ… Frontend integration\n\n---\n\n## ğŸš€ The Complete E2E Flow:\n\n```\nğŸ“„ PDF Upload\n    â†“\nğŸ“ Text Extraction (PDFLoader)\n    â†“\nâœ‚ï¸ Chunking (RecursiveTextSplitter)\n    â†“\nğŸ§® Embeddings (Azure OpenAI / Ollama)\n    â†“\nğŸ’¾ Vector Storage (Cosmos DB / FAISS)\n    â†“\nğŸ‘¤ User Query\n    â†“\nğŸ” Vector Search (Top-K similar chunks)\n    â†“\nğŸ“‹ Context Assembly\n    â†“\nğŸ’¬ Load Chat History\n    â†“\nğŸ¤– LLM Generation (with streaming)\n    â†“\nğŸ“¡ Stream to Client (NDJSON)\n    â†“\nğŸ’¬ Display + Save to History\n```\n\n---\n\n## ğŸ’¡ Key Takeaways for Beginners:\n\n1. **RAG = Context + Generation**\n   - Retrieval finds relevant info\n   - Generation creates natural responses\n\n2. **Embeddings capture meaning**\n   - Similar meanings = similar vectors\n   - Enables semantic search\n\n3. **Streaming improves UX**\n   - User sees response immediately\n   - Feels 10x faster than waiting\n\n4. **Chat history enables conversations**\n   - Remember context\n   - Multi-turn dialogues\n\n5. **Start simple, optimize later**\n   - Basic retrieval works great\n   - Add complexity only when needed\n\n---\n\n## ğŸ”¬ Advanced Concepts Covered:\n\n- Vector indexing algorithms (DiskANN, HNSW, IVF)\n- Similarity metrics and their trade-offs\n- Chunk size vs overlap optimization\n- Prompt engineering techniques\n- Token management and cost optimization\n- Retrieval evaluation (Precision, Recall, MRR)\n- Streaming protocols (chunked encoding, SSE, WebSockets)\n- Session management strategies\n- History compression techniques\n\n---\n\n## ğŸ“š What's Next?\n\n### Experiment Ideas:\n\n1. **Try Different Configurations**:\n   - Chunk sizes (500, 1500, 3000)\n   - Top-K values (1, 3, 5, 10)\n   - Temperatures (0.0, 0.7, 1.5)\n\n2. **Test Edge Cases**:\n   - Very long documents\n   - Multiple file types\n   - Non-English queries\n   - Complex follow-ups\n\n3. **Measure Performance**:\n   - Retrieval latency\n   - Token costs\n   - Answer accuracy\n   - User satisfaction\n\n4. **Enhance the System**:\n   - Add file upload UI\n   - Implement feedback loops\n   - Add analytics dashboard\n   - Support more file types\n\n### Build Your Own RAG App:\n\nYou now have all the knowledge to:\n- âœ… Set up document ingestion\n- âœ… Configure vector storage\n- âœ… Implement retrieval\n- âœ… Generate responses\n- âœ… Stream to users\n- âœ… Manage chat sessions\n\n**Start building!** ğŸš€\n\n---\n\n## Return to the Build-a-thon\n\nThank you for completing this CodeTour for a comprehensive understanding of RAG implementation using LangChain.js and Azure serverless technologies. Now resume your quest progress on the main Build-a-thon repository.\n\n---\n\n### Happy Building! ğŸ‰"
    }
  ]
}