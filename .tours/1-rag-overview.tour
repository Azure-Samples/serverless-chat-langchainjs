{
  "$schema": "https://aka.ms/codetour-schema",
  "title": "1 - RAG Overview: Understanding the Architecture",
  "description": "Introduction to Retrieval-Augmented Generation (RAG) and how it powers this serverless chat application. Perfect for beginners and those new to RAG concepts.",
  "isPrimary": true,
  "nextTour": "2 - Document Ingestion: From PDFs to Vectors",
  "steps": [
    {
      "title": "Welcome to RAG",
      "description": "## Welcome! ğŸ‰\n\nThis tour will guide you through the **Retrieval-Augmented Generation (RAG)** architecture powering this serverless chat application.\n\n### What You'll Learn:\n- What RAG is and why it matters\n- The complete RAG pipeline from documents to answers\n- How LangChain.js orchestrates everything\n- Real implementation code you can explore\n\n### Prerequisites:\nâœ… You've cloned this repository\nâœ… Ollama is running locally\nâœ… The app is running (`npm start`)\n\n**Ready?** Let's start by understanding what RAG actually is!\n\n---\nğŸ’¡ **Tip**: This is tour 1 of 6. Each tour focuses on a specific part of the RAG pipeline."
    },
    {
      "file": "docs/tutorial/03-understanding-rag.md",
      "line": 7,
      "title": "What is RAG?",
      "description": "## What is RAG? ğŸ¤”\n\n**Retrieval-Augmented Generation (RAG)** combines two powerful capabilities:\n\n1. **Retrieval**: Finding relevant information from your documents\n2. **Generation**: Using a Language Model to create natural language responses\n\n### Why RAG?\n\nLanguage Models have vast general knowledge, but they:\n- Don't know about your specific documents or company data\n- Can't access real-time or proprietary information\n- May \"hallucinate\" when they don't have the right context\n\n**RAG solves this** by giving the model access to your specific documents at query time!\n\n### Real-World Example:\nâŒ **Without RAG**: \"What's Contoso Real Estate's pet policy?\"\n  â†’ Model guesses or says \"I don't know\"\n\nâœ… **With RAG**: System finds the relevant policy document, then model answers:\n  â†’ \"According to the terms of service, tenants may keep up to 2 pets...\"\n\n---\n**Next**: Let's see the three main components of RAG architecture."
    },
    {
      "file": "docs/tutorial/03-understanding-rag.md",
      "line": 17,
      "title": "RAG Architecture: The Three Pillars",
      "description": "## The Three Pillars of RAG ğŸ›ï¸\n\nEvery RAG system has three core components:\n\n### 1. ğŸ“¥ **Ingestion** (Preparation Phase)\n- Load documents (PDFs, text, etc.)\n- Split them into smaller chunks\n- Convert chunks into **embeddings** (vector representations)\n- Store in a **vector database**\n\n### 2. ğŸ” **Retrieval** (Search Phase)\n- Convert user question into an embedding\n- Search vector database for similar chunks\n- Rank and select most relevant documents\n\n### 3. ğŸ¤– **Generation** (Response Phase)\n- Inject retrieved documents into prompt\n- Send to the model with user question\n- Model generates answer based on context\n- Stream response back to user\n\n---\n\n### ğŸ’¡ Advanced Concept: Vector Similarity\nWhy vectors? Because we can measure semantic similarity mathematically!\n- \"dog\" and \"puppy\" are closer in vector space than \"dog\" and \"car\"\n- This lets us find relevant content even if exact words don't match\n\n---\n**Next**: Let's see this in action in our code!"
    },
    {
      "file": "README.md",
      "line": 35,
      "title": "Architecture Overview",
      "description": "## Our RAG Implementation ğŸ—ï¸\n\nThis application implements RAG using Azure serverless technologies:\n\n### Components:\n- **Frontend**: Lit web component (Static Web App)\n- **Backend**: Azure Functions with LangChain.js\n- **Vector DB**: Azure Cosmos DB for NoSQL (or local FAISS)\n- **Storage**: Azure Blob Storage (for original PDFs)\n- **LLM**: Azure OpenAI (or local Ollama)\n\n### Data Flow:\n```\nğŸ“„ PDF Upload â†’ ğŸ”ª Chunking â†’ ğŸ§® Embeddings â†’ ğŸ’¾ Vector DB\n                                                    â†“\nğŸ‘¤ User Query â†’ ğŸ” Vector Search â†’ ğŸ“‹ Context â†’ ğŸ¤– LLM â†’ ğŸ’¬ Answer\n```\n\n### Why Serverless?\n- **Cost-effective**: Pay only for what you use\n- **Scalable**: Handles 1 or 1000 users automatically\n- **Low maintenance**: No servers to manage\n\n---\n**Next**: Let's look at the two main API endpoints that power this system."
    },
    {
      "file": "packages/api/src/functions/documents-post.ts",
      "line": 102,
      "title": "The Two Core Endpoints",
      "description": "## Two Endpoints, Complete RAG ğŸ”Œ\n\nOur API has just two endpoints that implement the full RAG pipeline.\n\nHere's the **first endpoint** - document ingestion:\n\n```typescript\napp.http('documents-post', {\n  route: 'documents',\n  methods: ['POST'],\n  authLevel: 'anonymous',\n  handler: postDocuments,\n});\n```\n\n### 1. `POST /documents` - The Ingestion Pipeline\n**Route**: `/documents`\n\n**What it does:**\n- Receives PDF file upload\n- Extracts text using LangChain's PDFLoader\n- Splits into chunks (overlap for context continuity)\n- Generates embeddings for each chunk\n- Stores in vector database\n- Saves original PDF to blob storage\n\n**When to use:** When adding new documents to the knowledge base\n\n---\n\nAnd the **second endpoint** (in chats-post.ts) - retrieval + generation:\n\n```typescript\napp.http('chats-post', {\n  route: 'chats/stream',\n  methods: ['POST'],\n  authLevel: 'anonymous',\n  handler: postChats,\n});\n```\n\n### 2. `POST /chats/stream` - The Retrieval + Generation Pipeline\n**Route**: `/chats/stream`\n\n**What it does:**\n- Receives user message + chat history\n- Generates embedding for user question\n- Performs vector similarity search\n- Retrieves top-k most relevant chunks\n- Constructs prompt with context + question\n- Streams LLM response back to client\n- Maintains chat session history\n\n**When to use:** Every time the user sends a message\n\n---\n\n### ğŸ’¡ Beginner: What's `app.http()`?\n\nAzure Functions uses this to register HTTP endpoints:\n- **`route`**: The URL path (`/documents`, `/chats/stream`)\n- **`methods`**: HTTP methods allowed (`POST`)\n- **`handler`**: The function that processes requests\n\nThat's it! Azure handles the rest (scaling, routing, etc.)\n\n### ğŸ”¬ Advanced: Design Pattern\n\nNotice how **ingestion is separate from retrieval**?\n\nThis is a best practice:\n- âœ… Pre-process documents **once**\n- âœ… Query them efficiently **many times**\n- âœ… Independent scaling (upload vs query workloads)\n- âœ… Clear separation of concerns\n\n---\n**Next**: Let's see the document ingestion handler implementation!"
    },
    {
      "file": "packages/api/src/functions/documents-post.ts",
      "line": 19,
      "title": "Document Ingestion Function",
      "description": "## The Document Upload Handler ğŸ“¤\n\nThis is the entry point for the **ingestion pipeline**.\n\n### What happens here:\n1. Receives multipart form data (PDF file)\n2. Checks environment (Azure OpenAI vs local Ollama)\n3. Routes to appropriate ingestion pipeline\n\n### Key Decision Point:\n```typescript\nif (azureOpenAiEndpoint) {\n  // Use Azure OpenAI embeddings + Cosmos DB\n} else {\n  // Use Ollama embeddings + local FAISS\n}\n```\n\n### Why Two Paths?\n- **Azure path**: Production deployment with managed services\n- **Local path**: Cost-free local development with Ollama\n\n**Both implement the exact same RAG logic!**\n\n---\n\nğŸ’¡ **Beginner Tip**: The \"endpoint\" is just the URL users call. Azure Functions automatically creates this endpoint from our code.\n\nğŸ”¬ **Advanced Note**: This abstraction pattern (same interface, different implementations) is a key software design principle that makes testing easier.\n\n---\n**Next**: Now let's see the chat function that handles user queries!"
    },
    {
      "file": "packages/api/src/functions/chats-post.ts",
      "line": 42,
      "title": "Chat Query Handler",
      "description": "## The Chat Handler - Where RAG Happens ğŸ’¬\n\nThis is where the **retrieval + generation pipeline** comes together!\n\n### The RAG Flow:\n```\nUser Question â†’ Embedding â†’ Vector Search â†’ Retrieval â†’ LLM â†’ Streaming Response\n```\n\n### What this function does:\n1. âœ… Validates incoming messages\n2. ğŸ†” Manages user sessions\n3. ğŸ§® Initializes embeddings model\n4. ğŸ¤– Initializes chat model (LLM)\n5. ğŸ’¾ Connects to vector store\n6. ğŸ“œ Loads chat history\n7. â›“ï¸ Creates LangChain chains\n8. ğŸ” Performs retrieval\n9. ğŸ¯ Generates response\n10. ğŸ“¡ Streams back to client\n\n### Notice the Pattern:\nSame as document uploadâ€”we check for Azure vs local setup, then execute the same logical flow.\n\n---\n\n**Next sections** of this tour will walk through:\n- How retrieval works (vector search)\n- How generation works (LLM chains)\n- How streaming delivers real-time responses\n\n---\n**Next**: Let's understand how RAG chains work in LangChain.js!"
    },
    {
      "file": "packages/api/src/functions/chats-post.ts",
      "line": 14,
      "title": "The RAG System Prompt",
      "description": "## The System Prompt - RAG's Secret Sauce ğŸ¯\n\nThis prompt is **critical** to RAG's success!\n\n### What Makes a Good RAG Prompt:\n\n#### 1. Clear Instructions\n```\n\"Answer ONLY with information from the sources below.\"\n```\nâ†’ Prevents hallucination!\n\n#### 2. Honesty Directive\n```\n\"If there isn't enough information, say you don't know.\"\n```\nâ†’ Better than wrong answers!\n\n#### 3. Source Attribution\n```\n\"ALWAYS reference the source filename.\"\n```\nâ†’ Users can verify information!\n\n#### 4. Follow-up Questions\n```\n\"Generate 3 very brief follow-up questions.\"\n```\nâ†’ Better UX, guides conversation!\n\n#### 5. Context Injection Point\n```\n{context}  â† Retrieved documents go here\n```\n\n---\n\nğŸ’¡ **Beginner**: Think of this as instructions to a very literal assistant. Be specific!\n\nğŸ”¬ **Advanced**: Prompt engineering is an art. Small changes here can dramatically affect response quality. Consider:\n- Temperature settings (creativity vs consistency)\n- Token limits (cost vs completeness)\n- Citation formats (structured vs natural)\n\n---\n**Next**: See how the retrieval chain is constructed!"
    },
    {
      "file": "packages/api/src/functions/chats-post.ts",
      "line": 87,
      "title": "Building the RAG Chain",
      "description": "## LangChain Chains - Connecting the Pipeline ğŸ”—\n\nLangChain uses \"chains\" to connect different components. Here we build the core RAG chain:\n\n### Step 1: Document Chain\n```typescript\ncreateStuffDocumentsChain({\n  llm: model,           // The LLM to use\n  prompt: ChatPromptTemplate,  // System + user prompts\n  documentPrompt: PromptTemplate  // How to format each doc\n})\n```\n**\"Stuff\"** = stuff all retrieved documents into the prompt!\n\n### Step 2: Add Chat History\n```typescript\nRunnableWithMessageHistory({\n  runnable: ragChain,\n  getMessageHistory: () => chatHistory\n})\n```\nThis wraps our chain to remember previous messages!\n\n### Step 3: Create Retriever\n```typescript\nstore.asRetriever(3)  // Get top 3 most relevant docs\n```\n\n---\n\n### ğŸ’¡ Beginner: What's a Chain?\nThink of it like a recipe:\n1. Take user question\n2. Find relevant documents\n3. Combine them with question\n4. Send to LLM\n5. Get answer\n\nChains automate these steps!\n\n### ğŸ”¬ Advanced: Other Chain Types\n- **MapReduceDocumentsChain**: Process each doc separately, then combine\n- **RefineDocumentsChain**: Iteratively refine answer with each doc\n- **MapRerankDocumentsChain**: Score each doc's answer, pick best\n\n\"Stuff\" is simplest and works great for small context windows!\n\n---\n**Next**: See how retrieval actually happens!"
    },
    {
      "file": "packages/api/src/functions/chats-post.ts",
      "line": 112,
      "title": "The Retrieval Step",
      "description": "## Vector Search in Action ğŸ”\n\nThis is where RAG \"retrieves\" relevant information!\n\n### The Magic Line:\n```typescript\ncontext: await retriever.invoke(question)\n```\n\n### What Happens Under the Hood:\n1. **Question â†’ Embedding**\n   - User question converted to vector (array of numbers)\n   - Example: \"What's the pet policy?\" â†’ [0.23, -0.45, 0.67, ...]\n\n2. **Vector Similarity Search**\n   - Compare question vector to all document vectors in DB\n   - Find closest matches using cosine similarity\n   - Rank by relevance score\n\n3. **Top-K Selection**\n   - Retrieve top 3 most similar chunks (`.asRetriever(3)`)\n   - Return with metadata (source filename, page number)\n\n4. **Context Assembly**\n   - Format: `[filename]: content`\n   - Injected into the system prompt at `{context}`\n\n---\n\n### ğŸ’¡ Beginner: Why Vectors?\nVectors capture **meaning**, not just words:\n- \"car\" and \"automobile\" have similar vectors\n- \"bank\" (river) and \"bank\" (money) have different vectors\n- This is why RAG understands synonyms!\n\n### ğŸ”¬ Advanced: Retrieval Strategies\n- **Top-K**: Simple, fast (we use this)\n- **MMR (Maximal Marginal Relevance)**: Diverse results\n- **Similarity Threshold**: Only above certain score\n- **Hybrid Search**: Combine keyword + vector search\n\n---\n**Next**: See how the response is generated and streamed!"
    },
    {
      "file": "packages/api/src/functions/chats-post.ts",
      "line": 147,
      "title": "Streaming the Response",
      "description": "## Real-Time Streaming ğŸ“¡\n\nNotice we're using `.stream()` instead of `.invoke()`â€”this gives us real-time responses!\n\n### How Streaming Works:\n\n1. **LLM generates tokens** (words/pieces of words)\n2. **Each token immediately sent to client**\n3. **User sees response as it's being written**\n4. **Better UX** - no waiting for full response!\n\n### The Code:\n```typescript\nconst responseStream = await ragChainWithHistory.stream(\n  { input: question, context: retrieved_docs },\n  { configurable: { sessionId } }\n);\n```\n\n### Transform to JSON Stream:\n```typescript\nconst jsonStream = Readable.from(createJsonStream(responseStream, sessionId));\n```\n\nEach chunk is formatted as:\n```json\n{\"delta\": {\"content\": \"Hello\", \"role\": \"assistant\"}, \"context\": {\"sessionId\": \"...\"}}\n{\"delta\": {\"content\": \" world\", \"role\": \"assistant\"}, \"context\": {\"sessionId\": \"...\"}}\n```\n\n---\n\n### ğŸ’¡ Beginner: Why Stream?\nImagine asking a long question. Would you rather:\n- â° Wait 10 seconds, then see full answer\n- âœ¨ See answer appear word-by-word immediately\n\nStreaming = better perceived performance!\n\n### ğŸ”¬ Advanced: Streaming Benefits\n- **Lower latency**: First token in ~500ms vs full response in 5s\n- **Better error handling**: Can show partial response if stream breaks\n- **Token counting**: Monitor usage in real-time\n- **Cancellation**: User can stop generation mid-stream\n\n---\n**Next**: Let's wrap up with key takeaways!"
    },
    {
      "title": "Tour Complete - Next Steps",
      "description": "## ğŸ‰ You've Completed the RAG Overview!\n\n### What You've Learned:\nâœ… RAG combines retrieval + generation for accurate, grounded responses\nâœ… Three phases: Ingestion â†’ Retrieval â†’ Generation\nâœ… Vector embeddings enable semantic search\nâœ… LangChain chains orchestrate the pipeline\nâœ… Streaming provides real-time user experience\n\n---\n\n## ğŸ“š Continue Your Learning Journey:\n\nThis tour gave you the **big picture**. Now dive deeper into each component:\n\n### [Tour 2: Document Ingestion]\nLearn how PDFs become searchable vectors:\n- PDF loading and parsing\n- Text chunking strategies\n- Embedding generation\n- Vector storage\n\n### Tours 3-6:\n- **Tour 3**: Vector Storage (Cosmos DB & FAISS deep-dive)\n- **Tour 4**: Query & Retrieval (Search algorithms)\n- **Tour 5**: Response Generation (LLM chains & prompts)\n- **Tour 6**: Streaming & Chat History (Real-time UX)\n\n---\n\n## ğŸ”— Next Tour:\nClick below to start Tour 2 and learn how documents are processed!\n\n\n\n---\n\nğŸ’¡ **Tip**: You can restart any tour from the CodeTour panel in VS Code's Explorer sidebar!"
    }
  ]
}