{
  "$schema": "https://aka.ms/codetour-schema",
  "title": "2 - Document Ingestion: From PDFs to Vectors",
  "description": "Deep dive into how documents are processed, chunked, and converted into searchable embeddings. Learn the ingestion phase of RAG.",
  "nextTour": "3 - Vector Storage: Cosmos DB & FAISS Deep Dive",
  "steps": [
    {
      "title": "Document Ingestion Overview",
      "description": "## Welcome to Tour 2: Document Ingestion! üìÑ\n\n### What We'll Cover:\n1. How PDF files are loaded and parsed\n2. Why and how we split text into chunks\n3. What embeddings are and why they matter\n4. How vectors are stored in the database\n5. Local vs Azure implementation paths\n\n### The Ingestion Pipeline:\n```\nüìÑ PDF ‚Üí üìù Text Extraction ‚Üí üî™ Chunking ‚Üí üßÆ Embedding ‚Üí üíæ Vector DB\n```\n\n### Why This Matters:\n- Good chunking = better retrieval = better answers\n- Poor chunking = context loss = confused AI\n- Embeddings quality directly impacts search accuracy\n\n---\n\nüí° **Beginner Note**: Think of ingestion as preparing a cookbook index. Each recipe (document) is broken into sections (chunks), and each section is tagged with keywords (embeddings) so you can find it later!\n\nüî¨ **Advanced Preview**: We'll explore chunk size vs overlap trade-offs, embedding dimensionality, and storage optimization strategies.\n\n---\n**Ready?** Let's start with the document upload endpoint!"
    },
    {
      "file": "packages/api/src/functions/documents-post.ts",
      "line": 1,
      "title": "Required Dependencies",
      "description": "## Setting Up Document Processing üì¶\n\nThese imports provide everything needed for document ingestion:\n\n### Core Dependencies:\n\n**File System:**\n- `fs/promises` - Async file operations\n\n**Azure Functions:**\n- `HttpRequest`, `HttpResponseInit` - HTTP handling\n- `app` - Function registration\n\n**LangChain Loaders:**\n- `PDFLoader` - Extracts text from PDFs (uses pdf-parse under the hood)\n\n**Text Processing:**\n- `RecursiveCharacterTextSplitter` - Smart text chunking\n\n**Embeddings Models:**\n- `AzureOpenAIEmbeddings` - Azure OpenAI embeddings (text-embedding-3-small)\n- `OllamaEmbeddings` - Local Ollama embeddings (nomic-embed-text)\n\n**Vector Stores:**\n- `AzureCosmosDBNoSQLVectorStore` - Azure Cosmos DB with vector search\n- `FaissStore` - Local FAISS (Facebook AI Similarity Search)\n\n**Storage:**\n- `BlobServiceClient` - Azure Blob Storage for original PDFs\n\n---\n\n### üí° Beginner: What's an Embedding Model?\nA special AI model that converts text into numbers (vectors) that capture meaning. Similar meanings = similar numbers!\n\n### üî¨ Advanced: Model Choices\n- **Azure**: text-embedding-3-small (1536 dimensions, fast, accurate)\n- **Local**: nomic-embed-text (768 dimensions, runs on CPU)\n\nHigher dimensions = more nuance but more storage/compute cost."
    },
    {
      "file": "packages/api/src/functions/documents-post.ts",
      "line": 15,
      "title": "The Upload Handler",
      "description": "## Document Upload Entry Point üö™\n\nThis function handles the entire ingestion pipeline:\n\n### Input:\n- HTTP POST with multipart/form-data\n- Contains: PDF file + metadata\n\n### Output:\n- Success: \"Document uploaded successfully\"\n- Error: Appropriate error message + status code\n\n### The Two Paths:\nNotice the environment check:\n```typescript\nif (azureOpenAiEndpoint) {\n  // Path 1: Azure production path\n} else {\n  // Path 2: Local development path\n}\n```\n\nBoth paths do the SAME thing, just with different services!\n\n### Why This Pattern?\n- **Local**: Free, no Azure costs, perfect for learning\n- **Azure**: Production-ready, scalable, globally distributed\n- **Same code logic**: Easy to test locally, deploy to cloud\n\n---\n\n### üí° Beginner: What's multipart/form-data?\nA way to send files over HTTP. Think of it like attaching a file to an email!\n\n### üî¨ Advanced: Deployment Strategy\nThis dual-path pattern (local/cloud) is excellent for:\n- CI/CD: Test with Ollama, deploy with Azure OpenAI\n- Cost optimization: Dev locally, production in cloud\n- Vendor flexibility: Easy to swap providers\n\n---\n**Next**: Let's see how we parse the uploaded file!"
    },
    {
      "file": "packages/api/src/functions/documents-post.ts",
      "line": 22,
      "title": "Parsing the Upload",
      "description": "## Extracting the PDF File üìé\n\n### What's Happening:\n\n**1. Parse Form Data**\n```typescript\nconst parsedForm = await request.formData();\n```\nAzure Functions HTTP request contains multipart/form-data with the PDF.\n\n**2. Validate File Exists**\n```typescript\nif (!parsedForm.has('file')) {\n  return badRequest('\"file\" field not found in form data.');\n}\n```\nAlways validate! Users might forget to attach the file.\n\n**3. Extract File**\n```typescript\nconst file = parsedForm.get('file') as any as File;\nconst filename = file.name;\n```\n\n**Why the `as any as File`?**\nType mismatch between Node.js FormData and Azure Functions FormData APIs. The cast ensures TypeScript compiles.\n\n---\n\n### üí° Beginner: What's FormData?\n\nWhen you upload a file in a web browser:\n```html\n<form>\n  <input type=\"file\" name=\"file\" />\n  <button>Upload</button>\n</form>\n```\n\nThe browser sends it as `multipart/form-data` - a special format for files + text together.\n\n### üî¨ Advanced: FormData vs JSON\n\n**Why not just send JSON?**\n- ‚ùå JSON is text-based, binary files need base64 encoding (33% size increase)\n- ‚ùå Large PDFs would exceed JSON payload limits\n- ‚úÖ FormData streams binary data efficiently\n- ‚úÖ Built-in browser support\n\n**Alternative approaches:**\n- Pre-signed upload URLs (upload directly to Blob Storage)\n- Chunked uploads for very large files\n- Resumable uploads with retry logic\n\n---\n**Next**: See how PDFs are parsed!"
    },
    {
      "file": "packages/api/src/functions/documents-post.ts",
      "line": 33,
      "title": "PDF Text Extraction",
      "description": "## Loading the PDF üìñ\n\n### Breaking It Down:\n\n**1. Create Loader**\n```typescript\nconst loader = new PDFLoader(file, {\n  splitPages: false,  // Keep all pages as one document\n});\n```\n\n**Why `splitPages: false`?**\n- We'll do our own chunking in the next step\n- Gives us more control over chunk boundaries\n- Prevents page breaks from splitting sentences awkwardly\n\n**2. Extract Text**\n```typescript\nconst rawDocument = await loader.load();\n```\n\n**Returns an array** with one `Document` object (since we didn't split pages):\n```typescript\n[\n  {\n    pageContent: \"[All text from the entire PDF]...\",\n    metadata: {\n      pdf: {\n        version: \"1.4\",\n        info: { ... },\n        totalPages: 5\n      },\n      loc: { lines: { from: 1, to: 500 } }\n    }\n  }\n]\n```\n\n**3. Set Source Metadata**\n```typescript\nrawDocument[0].metadata.source = filename;\n```\n\nAdds the filename so we can cite it later!\n\n---\n\n### üí° Beginner: What's in pageContent?\n\nPDFLoader extracts pure text:\n```\nOriginal PDF:\n[Logo] Welcome to Contoso Real Estate\nTerms of Service...\n\npageContent:\n\"Welcome to Contoso Real Estate\\nTerms of Service...\"\n```\n\nNo images, no formatting - just text!\n\n### üî¨ Advanced: PDFLoader Internals\n\nUnder the hood, PDFLoader uses `pdf-parse` library:\n\n```typescript\nimport pdf from 'pdf-parse';\n\n// Simplified version of what happens:\nconst buffer = await file.arrayBuffer();\nconst data = await pdf(Buffer.from(buffer));\nconst text = data.text;  // All text extracted\n```\n\n**PDF Parsing Challenges:**\n- **Scanned PDFs**: No text layer ‚Üí Need OCR (not supported here)\n- **Tables**: Often parsed as gibberish ‚Üí Consider specialized parsers\n- **Multi-column layouts**: Can break reading order\n- **Images**: Lost (only text extracted)\n\n**For complex PDFs**, consider:\n- `UnstructuredPDFLoader` - Better layout preservation\n- `PDFPlumberLoader` - Better table handling  \n- Pre-process with OCR (Azure Document Intelligence)\n\n**Why LangChain's Document format?**\n\nLangChain uses this standard format across ALL loaders:\n- PDFLoader\n- TextLoader\n- CSVLoader\n- WebLoader\n- etc.\n\nThis means the rest of our pipeline is **loader-agnostic**! ‚ú®\n\n---\n**Next**: Now the critical step - chunking!"
    },
    {
      "file": "packages/api/src/functions/documents-post.ts",
      "line": 38,
      "title": "Text Chunking Strategy",
      "description": "## Splitting Documents into Chunks ‚úÇÔ∏è\n\nThis is one of the MOST IMPORTANT steps in RAG!\n\n### The Code:\n```typescript\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 1500,      // Max chunk size\n  chunkOverlap: 100,    // Overlap between chunks\n});\nconst documents = await splitter.splitDocuments(rawDocument);\n```\n\n### Why Chunk At All?\n\n**Problem**: A 50-page PDF is too large to:\n- Send to a model (token limits)\n- Match user queries precisely\n- Store as a single vector\n\n**Solution**: Split into smaller, focused chunks!\n\n### Chunk Size = 1500 characters\n- **Too small** (e.g., 200): Loses context, many chunks needed\n- **Too large** (e.g., 5000): Less precise retrieval, hits token limits\n- **Sweet spot** (1000-2000): Balances context + precision\n\n### Chunk Overlap = 100 characters\nWhy overlap? To avoid cutting sentences/paragraphs mid-thought!\n\n**Example:**\n```\nChunk 1: \"...the tenant must provide notice. The notice period is 30 days...\"\nChunk 2: \"...The notice period is 30 days. After notice, the landlord...\"\n          ‚Üë Overlap ensures context continuity\n```\n\n---\n\n### üí° Beginner: Think of It Like This\nReading a textbook: You don't memorize whole chapters, you study section by section. Chunks are those sections!\n\n### üî¨ Advanced: RecursiveCharacterTextSplitter\nWhy \"Recursive\"? It tries multiple separators in order:\n1. `\\n\\n` (paragraph breaks) ‚Üê Try first\n2. `\\n` (line breaks)\n3. ` ` (spaces)\n4. `\"\"` (characters) ‚Üê Last resort\n\nThis preserves natural language structure!\n\n**Alternative Splitters:**\n- `TokenTextSplitter` - Split by tokens (better for LLM limits)\n- `SemanticChunker` - Split by semantic similarity (newer, experimental)\n- `MarkdownTextSplitter` - Respects markdown structure\n\n---\n**Next**: The magic step - creating embeddings!"
    },
    {
      "file": "packages/api/src/functions/documents-post.ts",
      "line": 46,
      "title": "Generating Embeddings",
      "description": "## Creating Vector Embeddings üßÆ\n\n### What Are Embeddings?\n\n**Text ‚Üí Numbers that capture meaning**\n\nExample (simplified, real embeddings are 768-1536 dimensions):\n```\n\"dog\"     ‚Üí [0.8, 0.3, -0.1, ...]\n\"puppy\"   ‚Üí [0.75, 0.35, -0.05, ...] ‚Üê Similar!\n\"car\"     ‚Üí [-0.2, 0.9, 0.6, ...]  ‚Üê Different!\n```\n\n### The Code:\n```typescript\n// Azure path:\nconst embeddings = new AzureOpenAIEmbeddings({ azureADTokenProvider });\nawait AzureCosmosDBNoSQLVectorStore.fromDocuments(\n  documents,    // Our chunks\n  embeddings,   // Embedding model\n  { credentials }\n);\n```\n\n### What `.fromDocuments()` Does:\n1. **For each chunk**:\n   - Sends text to embedding model\n   - Gets back vector (array of floats)\n   - Stores: `{ text, vector, metadata }` in database\n\n2. **In batch** (efficient!):\n   - Groups chunks to minimize API calls\n   - Handles retries on failures\n   - Shows progress\n\n### Local Alternative:\n```typescript\nconst embeddings = new OllamaEmbeddings({ \n  model: ollamaEmbeddingsModel  // \"nomic-embed-text\"\n});\n```\n\nSame interface, different provider!\n\n---\n\n### üí° Beginner: Why This Works\nMagic of embeddings: Similar meanings = similar vectors!\n- \"What's the refund policy?\" finds chunks about refunds\n- Even if chunks use words like \"return\", \"money back\", etc.\n- No exact keyword matching needed!\n\n### üî¨ Advanced: Embedding Models Compared\n\n**Azure OpenAI (text-embedding-3-small)**\n- Dimensions: 1536\n- Speed: ~50ms per request (API)\n- Cost: $0.00002 per 1K tokens\n- Quality: Excellent for English\n\n**Ollama (nomic-embed-text)**\n- Dimensions: 768  \n- Speed: ~100ms (local GPU) to ~500ms (CPU)\n- Cost: Free!\n- Quality: Very good, optimized for retrieval\n\n**Key Insight**: More dimensions ‚â† always better. nomic-embed-text is specifically trained for retrieval tasks!\n\n---\n**Next**: See how vectors are stored!"
    },
    {
      "file": "packages/api/src/functions/documents-post.ts",
      "line": 53,
      "title": "Storing in Vector Database (Azure)",
      "description": "## Azure Cosmos DB Vector Storage ‚òÅÔ∏è\n\n### The Storage Call:\n```typescript\nawait AzureCosmosDBNoSQLVectorStore.fromDocuments(\n  documents,    // Chunked text\n  embeddings,   // Embedding model\n  { credentials } // Azure credentials\n);\n```\n\n### What Gets Stored:\nFor each chunk, Cosmos DB stores:\n```json\n{\n  \"id\": \"uuid\",\n  \"text\": \"The tenant must provide 30 days notice...\",\n  \"embedding\": [0.234, -0.456, 0.123, ...],  // 1536 floats\n  \"metadata\": {\n    \"source\": \"terms-of-service.pdf\",\n    \"page\": 3\n  }\n}\n```\n\n### Why Cosmos DB?\n\n‚úÖ **Vector search built-in**\n- Native similarity search (cosine, dot product, euclidean)\n- Indexed for fast queries\n\n‚úÖ **NoSQL flexibility**\n- Store any metadata\n- No rigid schema\n\n‚úÖ **Global distribution**\n- Replicate data worldwide\n- Low-latency access\n\n‚úÖ **Serverless tier**\n- Pay per request\n- Auto-scales\n\n---\n\n### üí° Beginner: What's a Vector Database?\nA specialized database optimized for:\n1. Storing high-dimensional vectors\n2. Finding similar vectors quickly\n3. Scaling to millions of vectors\n\nRegular databases (SQL) aren't designed for this!\n\n### üî¨ Advanced: Vector Indexing\n\nCosmos DB uses **DiskANN**:\n- Graph-based indexing\n- Sub-linear search time\n- ~95%+ recall at high speed\n\n**Trade-offs:**\n- Approximate (not exact) nearest neighbors\n- Index build time on large datasets\n- Storage overhead (~30% more than raw vectors)\n\n**Alternative Vector DBs:**\n- Pinecone (managed, vector-only)\n- Weaviate (open-source, schema-based)\n- pgvector (PostgreSQL extension)\n- Qdrant (Rust, very fast)\n\n---\n**Next**: See the local FAISS alternative!"
    },
    {
      "file": "packages/api/src/functions/documents-post.ts",
      "line": 57,
      "title": "Storing in FAISS (Local)",
      "description": "## Local Vector Storage with FAISS üíª\n\nFor local development, we use FAISS instead of Cosmos DB:\n\n### What's Different?\n\n**Cosmos DB**: Cloud, persistent, queryable from anywhere\n\n**FAISS**: Local files, must load into memory\n\n### The FAISS Pattern:\n1. **Check if index exists** (`folderExists`)\n2. **If exists**: Load ‚Üí Add documents ‚Üí Save\n3. **If not**: Create new ‚Üí Save\n\n### File Structure:\n```\n./.data/\n  faiss.index        # Vector index (binary)\n  docstore.json      # Document content & metadata\n```\n\n### Trade-offs:\n\n**FAISS Pros:**\n- ‚úÖ Free\n- ‚úÖ Very fast (in-memory)\n- ‚úÖ No network latency\n- ‚úÖ Great for development\n\n**FAISS Cons:**\n- ‚ùå Not persistent across restarts (must reload)\n- ‚ùå Single machine only\n- ‚ùå Memory-limited\n- ‚ùå Not production-ready\n\n---\n\n### üí° Beginner: Why Use Both?\n- **Development**: FAISS (free, fast setup)\n- **Production**: Cosmos DB (reliable, scalable)\n- **Same code**: Just swap the vector store!\n\n### üî¨ Advanced: FAISS Algorithms\n\nFAISS (Facebook AI Similarity Search) supports multiple index types:\n\n**IndexFlatL2** (what we use):\n- Brute force, exact search\n- Perfect for < 100K vectors\n- Memory: ~4 bytes √ó dimensions √ó count\n\n**IndexIVFFlat** (faster):\n- Inverted file index\n- Approximate search\n- Good for 100K-10M vectors\n\n**IndexHNSW** (fastest):\n- Hierarchical Navigable Small World\n- Best for 1M+ vectors\n- More memory, incredible speed\n\nFor local dev with small datasets, Flat is perfect!\n\n---\n**Next**: Blob Storage for Originals"
    },
    {
      "file": "packages/api/src/functions/documents-post.ts",
      "line": 69,
      "title": "Blob Storage for Originals",
      "description": "## Preserving Original Documents üóÑÔ∏è\n\nAfter storing vectors, we also save the original PDF to Azure Blob Storage:\n\n### Breaking It Down:\n\n**1. Check Configuration**\n```typescript\nif (storageUrl && containerName) {\n```\nOnly upload if Azure Blob Storage is configured (optional in local dev).\n\n**2. Get Azure Credentials**\n```typescript\nconst credentials = getCredentials();\nconst blobServiceClient = new BlobServiceClient(storageUrl, credentials);\n```\nUses Managed Identity in Azure, DefaultAzureCredential locally.\n\n**3. Navigate to Container**\n```typescript\nconst containerClient = blobServiceClient.getContainerClient(containerName);\nconst blockBlobClient = containerClient.getBlockBlobClient(filename);\n```\n\n**Hierarchy:**\n```\nStorage Account\n  ‚îî‚îÄ‚îÄ Container (like a folder)\n      ‚îî‚îÄ‚îÄ Blob (the file)\n```\n\n**4. Upload File**\n```typescript\nconst buffer = await file.arrayBuffer();\nawait blockBlobClient.upload(buffer, file.size, {\n  blobHTTPHeaders: { blobContentType: 'application/pdf' },\n});\n```\n\n**Why set `blobContentType`?**\n- Browser knows to open it as PDF (not download as binary)\n- Enables preview in Azure Portal\n- Proper MIME type for downloads\n\n---\n\n### Why Store Original PDFs?\n\n**We already have the text in chunks, so why keep the PDF?**\n\n1. **Audit trail**: Prove what was uploaded and when\n2. **Re-ingestion**: Can re-process with improved chunking strategies\n3. **Download**: Let users access original documents\n4. **Compliance**: Legal requirements for document retention\n5. **Backup**: If vector DB corrupted, can restore from originals\n\n### Architecture:\n```\nüìÑ PDF Upload\n    ‚Üì\n    ‚îú‚îÄ‚Üí üíæ Blob Storage (original file)\n    ‚îÇ\n    ‚îî‚îÄ‚Üí üìù Extract ‚Üí üî™ Chunk ‚Üí üßÆ Embed ‚Üí üíæ Vector Store (searchable)\n```\n\n**Blob Storage**: Original PDFs\n**Vector Store**: Searchable chunks + embeddings\n\n---\n\n### üí° Beginner: What's Blob Storage?\n\n\"Blob\" = **Binary Large Object** (fancy term for files)\n\nAzure Blob Storage = Cloud file storage, like Dropbox for applications!\n\n**Storage tiers:**\n- **Hot**: Frequent access, higher cost\n- **Cool**: Infrequent access (> 30 days)\n- **Cold**: Rare access (> 90 days)\n- **Archive**: Long-term backup (rare retrieval)\n\n### üî¨ Advanced: Storage Costs & Trade-offs\n\n**Scenario**: 1000 PDFs, ~1MB each = 1GB\n\n**Blob Storage** (Cool tier): ~$0.01/GB/month\n- 1GB PDFs = **$0.01/month** ‚Üê Very cheap!\n- Plus operations: ~$0.01 per 10K reads\n\n**Cosmos DB** (Serverless): Pay per request\n- Chunks: ~5000 chunks √ó 1.5KB = ~7.5MB text\n- Vectors: 5000 √ó 1536 dims √ó 4 bytes = ~30MB\n- Storage: ~$0.25/GB = ~$0.01/month\n- **But**: Request Units (RUs) cost dominates!\n  - Vector search: ~10 RU per query\n  - 10K queries = 100K RUs = ~$2.50\n\n**Key Insights:**\n- Blob storage is **incredibly cheap** for large files\n- Vector DB costs are in **compute (queries)**, not storage\n- Original files cost < 1% of vector search costs\n\n**Security Note:**\nWe use:\n```typescript\nconst credentials = getCredentials();\n```\n\nThis returns **Managed Identity** credentials in Azure:\n- No connection strings in code\n- No secrets to manage\n- Automatic credential rotation\n- Azure handles security ‚ú®\n\n---\n**Next**: Review what we've learned!"
    },
    {
      "title": "Ingestion Complete! üéâ",
      "description": "## Tour 2 Complete - Document Ingestion Mastered! ‚úÖ\n\n### What You've Learned:\n\n#### The Ingestion Pipeline:\n```\nüìÑ PDF Upload\n  ‚Üì\nüìù Text Extraction (PDFLoader)\n  ‚Üì  \n‚úÇÔ∏è Chunking (1500 chars, 100 overlap)\n  ‚Üì\nüßÆ Embeddings (text ‚Üí vectors)\n  ‚Üì\nüíæ Vector Storage (Cosmos DB / FAISS)\n  ‚Üì\nüóÑÔ∏è Original Storage (Blob Storage)\n```\n\n#### Key Concepts:\n\n**Beginner Level:**\n‚úÖ PDFs are converted to searchable text chunks\n‚úÖ Embeddings turn text into numbers that capture meaning\n‚úÖ Vector databases enable semantic search\n‚úÖ Metadata preserves source/page info for citations\n\n**Advanced Level:**\n‚úÖ Chunk size/overlap trade-offs (1500/100 is sweet spot)\n‚úÖ RecursiveCharacterTextSplitter preserves structure\n‚úÖ Embedding dimensions (1536 vs 768) affect accuracy/speed\n‚úÖ FAISS vs Cosmos DB for dev/prod environments\n‚úÖ DiskANN indexing for sub-linear search\n\n---\n\n## üöÄ What's Next?\n\nNow that documents are ingested, how do we STORE them efficiently?\n\n### [Tour 3: Vector Storage Deep Dive]\nExplore:\n- How Cosmos DB indexes vectors\n- FAISS index types and algorithms  \n- Similarity metrics (cosine vs dot product)\n- Storage optimization strategies\n- When to use which vector store\n\n---\n\n## üí° Practice Ideas:\n\n1. **Experiment with chunk sizes**: Try 500, 1500, 3000\n2. **Test different PDFs**: Simple text vs complex layouts\n3. **Compare embeddings**: Azure vs Ollama quality\n4. **Monitor costs**: Check Cosmos DB request units\n\n---\n\n### Continue Learning:\n"
    }
  ]
}
